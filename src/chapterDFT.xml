    <section>
        <title>DFT</title>
        <section>
            <title>Introduction</title>
            <p>
                Good books about DFT are \cite{DFT} and \cite{martin}, but they both contain
much more topics which we don't need and some topics are missing in each of
them, so this chapter gives a self-contained explanation of all one has to know
about a many body quantum mechanics and the DFT in order to be able to do DFT
calculations.
            </p>
        </section>
        <section>
            <title>Many Body Schr&ouml;dinger Equation</title>
            <p>
We use the Born-Oppenheimer approximation, which says that the nuclei of the
treated atoms are seen as fixed. A stationary electronic state (for <m>N</m>
electrons) is then described by a wave function 
<m>\Psi({\bf r_1},{\bf r_2},\cdots,{\bf r}_N)</m> 
fulfilling the many-body Schr&ouml;dinger equation
<e>\hat H\ket\Psi=(\hat T+\hat U+\hat V)\ket\Psi=E\ket\Psi</e>
where
<e>\hat T = \sum_i^N -{1\over2}\nabla_i^2</e>
is the kinetic term,
<e>\hat U = \sum_{i&lt;j}U({\bf r_i},{\bf r_j})=
{1\over2}\sum_{i,j}U({\bf r_i},{\bf r_j})</e>
<e>U({\bf r_i},{\bf r_j})=U({\bf r_j},{\bf r_i})={1\over|{\bf r_i}-{\bf r_j}|}
</e>
is the electron-electron interaction term and
<e>\hat V = \sum_i^N v({\bf r_i})</e>
<e>v({\bf r_i})=\sum_k -{Z_k\over|{\bf r_i}-{\bf R_k}|}</e>
is the interaction term between electrons and nuclei,
where <m>R_k</m> are positions of nuclei and <m>Z_k</m> the number of nucleons in each
nucleus (we are using atomic units). So for one atomic calculation with the
atom nucleus in the origin, we have
just <m>v({\bf r_i})=-{Z\over|{\bf r_i}|}</m>.
</p>
<p>
    <m>|\Psi|^2=\Psi^*\Psi</m> gives the probability density of measuring the first
    electron at the position <m>\bf r_1</m>, the second at <m>\bf r_2</m>, \dots and the Nth
    electron at the position <m>{\bf r}_N</m>. The normalization is such that 
    <m>\int |\Phi|^2\d^3 r_1\d^3 r_2\dots\d^3 r_N=1</m>.
    The <m>\Psi</m> is antisymmetric, i.e. 
    <m>
\Psi({\bf r_1},{\bf r_2},\cdots,{\bf r}_N)=
-\Psi({\bf r_2},{\bf r_1},\cdots,{\bf r}_N)=
-\Psi({\bf r_1},{\bf r}_N,\cdots,{\bf r_2})
</m> etc.
</p>
<p>
Integrating <m>|\Psi|^2</m> over the first <m>N-1</m> electrons is the probability
density that the <m>N</m>th electron is at the position <m>{\bf r}_N</m>.  Thus the
probability density <m>n({\bf r})</m> that any of the N electrons (i.e the first, or
the second, or the third, \dots, or the <m>N</m>th) is at the position <m>\bf r</m> is
called the particle (or charge or electron) density and is therefore given by:
<e>n({\bf r})= \int
\Psi^*({\bf r},{\bf r}_2,\cdots,{\bf r}_N)
\Psi  ({\bf r},{\bf r}_2,\cdots,{\bf r}_N)
\,\d^3 r_2\,\d^3 r_3\cdots\d^3 r_N+ </e>
<e>+\int
\Psi^*({\bf r}_1,{\bf r},\cdots,{\bf r}_N)
\Psi  ({\bf r}_1,{\bf r},\cdots,{\bf r}_N)
\,\d^3 r_1\,\d^3 r_3\cdots\d^3 r_N+\cdots</e>
<e>+\int
\Psi^*({\bf r}_1,{\bf r}_2,\cdots,{\bf r})
\Psi  ({\bf r}_1,{\bf r}_2,\cdots,{\bf r})
\,\d^3 r_1\,\d^3 r_2\,\d^3 r_3\cdots\d^3 r_{N-1}=</e>
<e>=\int(\delta({\bf r}-{\bf r}_1)+\delta({\bf r}-{\bf
r}_2)+\cdots+\delta({\bf r}-{\bf r}_N))</e>
<e>
\Psi^*({\bf r}_1,{\bf r}_2,\cdots,{\bf r}_N)
\Psi  ({\bf r}_1,{\bf r}_2,\cdots,{\bf r}_N)
\,\d^3 r_1\,\d^3 r_2\,\d^3 r_3\cdots\d^3 r_{N}=</e>
<e>=\sum_{i=1}^N\int
\braket{\Psi|{\bf r}_1,{\bf r}_2,\cdots,{\bf r}_N}\delta({\bf r}-{\bf r}_i)
\braket{{\bf r}_1,{\bf r}_2,\cdots,{\bf r}_N|\Psi}
\,\d^3 r_1\,\d^3 r_2\,\d^3 r_3\cdots\d^3 r_{N}=</e>
<e>=N\int
\braket{\Psi|{\bf r}_1,{\bf r}_2,\cdots,{\bf r}_N}\delta({\bf r}-{\bf r}_1)
\braket{{\bf r}_1,{\bf r}_2,\cdots,{\bf r}_N|\Psi}
\,\d^3 r_1\,\d^3 r_2\,\d^3 r_3\cdots\d^3 r_{N}=</e>
<e id="chargedensity">=N\int
\Psi^*({\bf r},{\bf r}_2,\cdots,{\bf r}_N)
\Psi  ({\bf r},{\bf r}_2,\cdots,{\bf r}_N)
\,\d^3 r_2\,\d^3 r_3\cdots\d^3 r_N</e>
Thus <m>\int_\Omega n({\bf r})\,\d^3r</m> gives the number of particles (and also the
amount of charge) in the region of integration <m>\Omega</m>. Obviously 
<m>\int n({\bf r})\,\d^3r=N</m>.
</p>
<p>
The energy of the system is given by
<e id="Emanybody">E=\braket{\Psi|\hat H|\Psi}=
\braket{\Psi|\hat T|\Psi}+\braket{\Psi|\hat U|\Psi}+\braket{\Psi|\hat V|\Psi}=
T+U+V</e>
where 
<e>T=\braket{\Psi|\hat T|\Psi}=\sum_i^N\int
\Psi^*({\bf r_1},{\bf r_2},\cdots,{\bf r_N})(-\half\nabla_i^2)
\Psi({\bf r_1},{\bf r_2},\cdots,{\bf r_N})\,\d^3 r_1\,\d^3 r_2\cdots\d^3 r_N
</e>
<e>U=\braket{\Psi|\hat U|\Psi}</e>
<e>V=\braket{\Psi|\hat V|\Psi}=\sum_i^N\int
\Psi^*({\bf r_1},{\bf r_2},\cdots,{\bf r_N})v({\bf r_i})
\Psi({\bf r_1},{\bf r_2},\cdots,{\bf r_N})\,\d^3 r_1\,\d^3 r_2\cdots\d^3 r_N=
</e>
<e>=\sum_i^N\int
\Psi^*({\bf r_1},{\bf r_2},\cdots,{\bf r_N})v({\bf r_1})
\Psi({\bf r_1},{\bf r_2},\cdots,{\bf r_N})\,\d^3 r_1\,\d^3 r_2\cdots\d^3 r_N=
</e>
<e>=N\int
\Psi^*({\bf r_1},{\bf r_2},\cdots,{\bf r_N})v({\bf r_1})
\Psi({\bf r_1},{\bf r_2},\cdots,{\bf r_N})\,\d^3 r_1\,\d^3 r_2\cdots\d^3 r_N=
</e>
<e id="V[n]">=\int v({\bf r}) n({\bf r})\d^3 r=V[n]</e>

It needs to be stressed, that <m>E</m> generally is <em>not</em> a functional of <m>n</m>
alone, only the <m>V[n]</m> is. In the next section we show however, that if 
the <m>\ket{\Psi}</m> is a ground state (of any system), then <m>E</m> becomes a
functional of <m>n</m>.
            </p>
        </section>

        <section>
            <title> The Hohenberg-Kohn Theorem </title>
            <p>
The SE gives the map
<e>C: V \to \Psi</e>
where <m>\Psi</m> is the ground state.
C is bijective (one-to-one correspondence), because to every <m>V</m> we can compute the corresponding <m>\Psi</m>
from SE and two different <m>V</m> and <m>V'</m> (differing by more than a constant) give
two different <m>\Psi</m>, because if <m>V</m> and <m>V'</m> gave the same <m>\Psi</m>, then by
substracting
<e>\hat H\ket{\Psi}=E_{gs}\ket{\Psi}</e>
from
<e>\hat H'\ket{\Psi}=(\hat H-\hat V+\hat V')\ket{\Psi}=E_{gs}'\ket{\Psi}</e>
we would get <m>V-V'=E-E'</m>, which is a contradiction with the assumption that <m>V</m>
and <m>V'</m> differ by more than a constant.
</p>
<p>
Similarly, from the ground state wavefunction <m>\Psi</m> we can compute the charge
density <m>n</m> giving rise to the
map
<e>D: \Psi \to n</e> which is also bijective, because to every <m>\Psi</m> we can
compute <m>n</m> from <a ref="chargedensity"/> and two different <m>\Psi</m> and <m>\Psi'</m> give
two different <m>n</m> and <m>n'</m>, because different <m>\Psi</m> and <m>\Psi'</m> give
<e>E_{gs}=\braket{\Psi|\hat H|\Psi}&lt;\braket{\Psi'|\hat H|\Psi'}=
\braket{\Psi'|\hat H'+\hat V-\hat V'|\Psi'}=E_{gs}'+\int n'({\bf r})
(v({\bf r})-v'({\bf r}))\,\d^3 r</e>
<e>E_{gs}'=\braket{\Psi'|\hat H'|\Psi'}&lt;\braket{\Psi|\hat H'|\Psi}=
\braket{\Psi|\hat H+\hat V'-\hat V|\Psi}=E_{gs}+\int n({\bf r})
(v'({\bf r})-v({\bf r}))\,\d^3 r</e>
adding these two inequalities together gives
<e>0&lt;\int n'({\bf r}) (v({\bf r})-v'({\bf r}))\,\d^3 r + 
\int n({\bf r}) (v'({\bf r})-v({\bf r}))\,\d^3 r=
\int (n({\bf r})-n'({\bf r}))(v'({\bf r})-v({\bf r}))\,\d^3 r</e>
which for <m>n=n'</m> gives <m>0&lt;0</m>, which is nonsense, so <m>n\neq n'</m>.
</p>
<p>
So we have proved that for a given ground state density <m>n_0({\bf r})</m>
(generated by a potential <m>\hat V_0</m>)
it is possible to calculate the corresponding ground state wavefunction
<m>\Psi_0({\bf r_1},{\bf r_2},\cdots,{\bf r_N})</m>, in other words,
<m>\Psi_0</m> is a unique functional of <m>n_0</m>:
<e>\Psi_0=\Psi_0[n_0]</e>
so the ground state energy <m>E_0</m> is also a functional of <m>n_0</m>
<e>E_0=\braket{\Psi_0[n_0]|\hat T+\hat U+\hat V_0|\Psi_0[n_0]}=E[n_0]</e>
We define an energy functional
<e id="Efunct">E_{v_0}[n]=\braket{\Psi[n]|\hat T+\hat U+\hat V_0|\Psi[n]}=
\braket{\Psi[n]|\hat T+\hat U|\Psi[n]}+\int v_0({\bf r})n({\bf r})\d^3r
</e>
where <m>\ket{\Psi[n]}</m> is any ground state wavefunction (generated by an
arbitrary potential), that is, <m>n</m> is a ground state density belonging to an
arbitrary system. <m>E_0</m> which is generated by the potential <m>V_0</m> can then be
expressed as
<e>E_0=E_{v_0}[n_0]</e>
and for <m>n\neq n_0</m> we have (from the Ritz principle)
<e>E_0&lt;E_{v_0}[n]</e>
and one has to minimise the functional <m>E_{v_0}[n]</m>:
<e id="Emin">E_0=\min_n E_{v_0}[n]</e>
The term 
<e>\braket{\Psi[n]|\hat T+\hat U|\Psi[n]}\equiv F[n]</e>
in <a ref="Efunct"/> is universal in the sense that it doesn't depend on <m>\hat V_0</m>.
It can be proven \cite{DFT}, that <m>F[n]</m> is a functional of <m>n</m> for degenerated
ground states too, so <a ref="Emin"/> stays true as well.
</p>
<p>
The ground state densities in <a ref="Efunct"/> and <a ref="Emin"/> are called
<em>pure-state v-representable</em> because they are the densities of (possible
degenerate) ground state of the Hamiltonian with some local potential 
<m>v({\bf r})</m>. One may ask a question if all possible functions are
v-representable (this is called the v-representability problem). The question
is relevant, because we need to know which functions to take into account in
the minimisation proccess <a ref="Emin"/>. Even though not every function is
v-representable \cite{DFT}, every density defined on a grid (finite of
infinite) which is strictly positive, normalized and consistent with the Pauli
principle is ensemble v-representable. Ensemble v-representation is just a
simple generalization of the above, for details see \cite{DFT}. In plain words,
we are fine.
</p>
<p>
The functional <m>E_{v_0}[n]</m> in <a ref="Emin"/> depends on the particle number <m>N</m>,
so in order to get <m>n</m>, we need to solve the variational formulation
<e>{\delta\over\delta n}\left(E_v[n]-\mu(N)\int n(\bf r)\d^3r\right)=0</e>
so
<e id="euler">{\delta E_v[n]\over\delta n}=\mu(N)</e>
Let the <m>n_N(\bf r)</m> be the solution of <a ref="euler"/> with a particle number <m>N</m>
and the energy <m>E_N</m>:
<e>E_N=E_v[n_N]</e>
The Lagrangian multiplier <m>\mu</m> is the exact chemical potential of the system
<e>\mu(N)={\partial E_N\over\partial N}</e>
becuase
<e>E_{N+\epsilon}-E_N=E_v[n_{N+\epsilon}]-E_v[n_N]
=\int {\delta E_v\over\delta n} (n_{N+\epsilon}-n_N)\d^3r=
</e>
<e>=\int \mu(N) (n_{N+\epsilon}-n_N)\d^3r
=\mu(N)(N+\epsilon-N)=\mu(N)\epsilon</e>
so
<e>\mu(N)={E_{N+\epsilon}-E_N\over\epsilon}
\ \longrightarrow \
{\partial E_N\over\partial N}</e>
            </p>
        </section>
        <section>
            <title> The Kohn-Sham Equations </title>
            <p>
Consider an auxiliary system of <m>N</m> noninteracting electrons (noninteracting
gas):
<e>\hat H_s=\hat T+\hat V_s</e>
Then the many-body ground state wavefunction can be decomposed into single particle orbitals
<e>\ket{\Psi ({\bf r_1},{\bf r_2},\cdots,{\bf r_N})}=
\ket{\psi_1({\bf r})}\ket{\psi_2({\bf r})}\cdots\ket{\psi_N({\bf r})}</e>
and
<e>E_s[n]=T_s[\{\psi_i[n]\}]+V_s[n]</e>
where
<e>T_s[n]=\braket{\Psi[n]|\hat T|\Psi[n]}=
\sum_i\braket{\psi_i|-\half\nabla^2|\psi_i}</e>
<e>V_s[n]=\braket{\Psi[n]|\hat V|\Psi[n]}=\int v_s({\bf r})n({\bf r})\d^3r</e>
From <a ref="euler"/> we get
<e id="noninteract">\mu={\delta E_s[n]\over\delta n({\bf r})}=
{\delta T_s[n]\over\delta n({\bf r})}+{\delta V_s[n]\over\delta n({\bf r})}=
{\delta T_s[n]\over\delta n({\bf r})}+v_s({\bf r})
</e>
Solution to this equation gives the density <m>n_s</m>.
</p>
<p>
Now we want to express the energy in <a ref="Emanybody"/> using <m>T_s</m> and <m>E_H</m> for
convenience, 
where <m>E_H</m> is the classical electrostatic interaction energy of the charge
distribution <m>n({\bf r})</m>:
<e>\nabla^2 V_H=n({\bf r})</e>
or uquivalently
<e>E_H[n]=\half\int\int {n({\bf r})n({\bf r'})\over|{\bf r}-{\bf r'}|}
\d^3r\d^3r'</e>
<e id="V_H">V_H({\bf r})={\delta E_H\over\delta n({\bf r})}=\half\int 
{n({\bf r'})\over|{\bf r}-{\bf r'}|} \d^3r'</e>
So from <a ref="Efunct"/> we get 
<e>E[n]=(T+U)[n]+V[n]=T_s[n]+E_H[n]+(T-T_s+U-E_H)[n]+V[n]=</e>
<e id="Efunctxc">=T_s[n]+E_H[n]+E_{xc}[n]+V[n]</e>
The rest of the energy is denoted by <m>E_{xc}=U-E_H+T-T_s</m> and it is called is the
exchange and correlation energy functional.
From <a ref="euler"/>
<e>\mu={\delta E[n]\over\delta n({\bf r})}=
{\delta T_s[n]\over\delta n({\bf r})}+
{\delta E_H[n]\over\delta n({\bf r})}+
{\delta E_{xc}[n]\over\delta n({\bf r})}+
{\delta V[n]\over\delta n({\bf r})}
</e>
From <a ref="V_H"/> we have
<e>{\delta E_H\over\delta n({\bf r})}=V_H({\bf r})</e>
from <a ref="V[n]"/> we get
<e>{\delta V[n]\over\delta n({\bf r})}=v({\bf r})</e>
we define 
<e id="Vxcpot">{\delta E_{xc}[n]\over\delta n({\bf r})}=V_{xc}({\bf r})</e>
so we arrive at
<e id="interact">\mu={\delta E[n]\over\delta n({\bf r})}=
{\delta T_s[n]\over\delta n({\bf r})}+V_H({\bf r})+V_{xc}({\bf r})+v({\bf r})
</e>
Solution to this equation gives the density <m>n</m>.
Comparing <a ref="interact"/> to <a ref="noninteract"/> we see that if we choose
<e>v_s\equiv V_H+V_{xc}+v</e>
then <m>n_s({\bf r})\equiv n({\bf r})</m>.
So we solve the Kohn-Sham equations of this auxiliary non-interacting system
<e id="KSeq">(-\half\nabla^2+v_s({\bf r}))\psi_i({\bf r})
\equiv(-\half\nabla^2+V_H({\bf r})+V_{xc}({\bf r})+v({\bf r}))\psi_i({\bf r})
=\epsilon_i\psi({\bf r})</e>
which yield the orbitals <m>\psi_i</m> that reproduce the density <m>n({\bf r})</m> of
the original interacting system
<e id="KSdensity">n({\bf r})\equiv n_s({\bf r})=\sum_i^N|\psi_i({\bf r})|^2</e>
The sum is taken over the lowest <m>N</m> energies. Some of the <m>\psi_i</m> can be
degenerated, but it doesn't matter - the index <m>i</m> counts every eigenfunction
including all the degenerated.

In plain words, the trick is in realising, that the ground state energy can be
found by minimising the energy functional <a ref="Efunct"/> and in rewriting this
functional into the form <a ref="Efunctxc"/>, which shows that the interacting
system can be treated as a noninteracting one with a special potential.
            </p>
        </section>
        <section>
            <title>The XC Term</title>
            <p>
The exchange and correlation functional
<e>E_{xc}[n]=(T+U)[n]-E_H[n]-T_S[n]</e>
can always be written in the form
<e>E_{xc}[n]=\int n({\bf r}')\epsilon_{xc}({\bf r}';n)\d^3r'</e>
where the <m>\epsilon_{xc}({\bf r}';n)</m> is called the xc energy density. 
</p>
<p>
Unfortunately, no one knows <m>\epsilon_{xc}({\bf r}';n)</m> exactly (yet).  The
most simple approximation is the <em>local density approximation</em> (LDA),
for which the xc energy density <m>\epsilon_{xc}</m> at <m>\bf r</m> is taken as that of
a homogeneous electron gas (the nuclei are replaced by a uniform positively
charged background, density <m>n=\rm const</m>) with the same local density:
<e>\epsilon_{xc}({\bf r};n)\approx\epsilon_{xc}^{LD}(n({\bf r}))</e>
</p>
<p>
The xc potential <m>V_{xc}</m> defined by <a ref="Vxcpot"/> is then
<e>V_{xc}({\bf r};n)={\delta E_{xc}[n]\over\delta n({\bf r})}=
\epsilon_{xc}({\bf r}';n)+
\int n({\bf r}'){\delta \epsilon_{xc}({\bf r}';n)\over\delta n({\bf r})}\d^3r'
</e>
which in the LDA becomes
<e id="Vxcld">V_{xc}({\bf r};n)
=\epsilon_{xc}^{LD}(n)+n{\d \epsilon_{xc}^{LD}(n)\over \d n}=
{\d \over \d n}\left(n\epsilon_{xc}^{LD}(n)\right)=
V_{xc}^{LD}(n)</e>
The xc energy density <m>\epsilon_{xc}^{LD}</m> of the homogeneous gas can be
computed exactly\cite{martin}: 
<e>\epsilon_{xc}^{LD}(n)=\epsilon_x^{LD}(n)+\epsilon_c^{LD}(n)</e>
where the <m>\epsilon_x^{LD}</m> is the electron gas exchange term given
by\cite{martin}
<e>\epsilon_x^{LD}(n)=-{3\over4\pi}(3\pi^2 n)^{1\over3}</e>
the rest of <m>\epsilon_{xc}^{LD}</m> is hidden in <m>\epsilon_c^{LD}(n)</m> for which
there doesn't exist an analytic formula, but the correlation energies are known
exactly from quantum Monte Carlo (QMC) calculations by Ceperley and
Alder\cite{pickett}. The energies were fitted by
Vosko, Wilkes and Nussair (VWN) with
<m>\epsilon_c^{LD}(n)</m> and they got accurate results with errors less than
<m>0.05\rm\,mRy</m> in <m>\epsilon_c^{LD}</m>, which means that <m>\epsilon_c^{LD}(n)</m> is
virtually known exactly. VWN result:
<e>\epsilon_c^{LD}(n)\approx {A\over2}\left\{
\ln\left(y^2\over Y(y)\right)+{2b\over Q}\arctan\left(Q\over 2y+b\right)+
\right.
</e>
<e>\left.
-{by_0\over Y(y_0)}\left[\ln\left((y-y_0)^2\over Y(y)\right)
+{2(b+2y_0)\over Q}\arctan\left(Q\over 2y+b\right)
\right] \right\}</e>
where <m>y=\sqrt{r_s}</m>, <m>Y(y)=y^2+by+c</m>, <m>Q=\sqrt{4c-b^2}</m>, <m>y_0=-0.10498</m>,
<m>b=3.72744</m>, <m>c=12.93532</m>, <m>A=0.0621814</m> and <m>r_s</m> is the electron gas
parameter, which gives the mean distance between electrons (in atomic units):
<e>r_s=\left(3\over4\pi n\right)^{1\over3}</e>
</p>
<p>
The xc potential is then computed from <a ref="Vxcld"/>:
<e>V_{xc}^{LD}=V_x^{LD}+V_c^{LD}</e>
<e>V_x^{LD}=-{1\over\pi}(3\pi^2 n)^{1\over3}</e>
</p>
<p>
<e>V_c^{LD}={A\over2}\left\{
\ln\left(y^2\over Y(y)\right)+{2b\over Q}\arctan\left(Q\over 2y+b\right)+
\right.
</e>
<e>\left.
-{by_0\over Y(y_0)}\left[\ln\left((y-y_0)^2\over Y(y)\right)
+{2(b+2y_0)\over Q}\arctan\left(Q\over 2y+b\right)
\right] \right\}+</e>
<e>
-{A\over6}{c(y-y_0)-by_0y\over (y-y_0)Y(y)}
</e>
</p>
<p>
Some people also use Perdew and Zunger formulas, but they give essentially the
same results.
The LDA, although very simple, is suprisingly successful. More sophisticated
approximations exist, for example the generalized gradient approximation (GGA),
which sometimes gives better results than the LDA, but is not perfect either.
Other options include orbital-dependent (implicit) density functionals or a
linear response type functionals, but this topic is still evolving. The
conclusion is, that the LDA is a good approximation to start with, and only
when we are not satisfied, we will have to try some more accurate and modern
approximation.
</p>
<p>
RLDA: Relativistic corrections to the energy-density functional were proposed
by MacDonald and Vosko and basically are just a change in 
<m>\epsilon_x^{LD}(n)\rightarrow\epsilon_x^{LD}(n)R</m>:
<e>R = 
\left[1-{3\over2}\left(\beta\mu-\ln(\beta+\mu)\over\beta^2\right)^2\right]</e>
where
<e>\mu=\sqrt{1+\beta^2}</e>
and
<e>\beta={(3\pi^2n)^{1\over3}\over c}</e>
We also need to calculate these derivatives:
<e>{\d R\over\d \beta}=
-6{\beta\mu-\ln(\beta+\mu)\over\beta^2}\left({1\over\mu}-
{\beta\mu-\ln(\beta+\mu)\over\beta^3}\right)
</e>
<e>{\d \beta\over\d n}={\beta\over 3n}</e>
<e>{\d \epsilon_x^{LD}\over\d n}={\epsilon_x^{LD}\over 3n}</e>
So
<e>V_x^{RLD}=\epsilon_x^{LD}R+n{\d \epsilon_x^{LD}R\over\d n}=
{4\over3}\epsilon_x^{LD}R+{1\over3}\epsilon_x^{LD}{\d R\over\d\beta}\beta</e>
For <m>c\to\infty</m> we get <m>\beta\to0</m>, <m>R\to1</m> and 
<m>V_x^{RLD}\to {4\over3}\epsilon_x^{LD}=V_x^{LD}</m> as expected, because
<e>\lim_{\beta\to0}{\beta\sqrt{1+\beta^2}-\ln(\beta+\sqrt{1+\beta^2})\over
\beta^2} = 0</e>
</p>
</section>
<section>
    <title> Iteration to Self-consistency</title>
    <p>

        The <m>V_H</m> and <m>V_{xc}</m> potentials in the Kohn-Sham equations <a
            ref="KSeq"/> depend
on the solution <m>n</m> thus the KS equations need to be iterated to obtain a 
self-consistent density. One can regard the KS procedure as a nonlinear operator
<m>\hat F</m> which satisfies (at the <m>M</m>th iteration)
<e>n_M^{\rm out}=\hat F n_M </e>
and the problem is to find the self-consistent density which satisfies
<e>n=\hat F n </e>
</p>
<p>
Due to the long-range nature of the Coulomb interaction, a small change in the
input density <m>n_M</m> can lead to a relatively large change in the output density
<m>\hat F n_M</m>, thus it is not possible to use the output density itself as the
input density for the next iteration, since large unstable charge oscillations
arise.  Rather it is essential to mix input and output densities in an
appropriate manner to obtain a new input density. 
</p>
<p>
The <m>n(\bf r)</m> is in practice defined on some grid, or using
coefficients of plane waves, local orbitals or the like, which means that the
precise relation
<e>\one=\int \ket{\bf r}\bra{\bf r}\d^3 r</e> is 
changed for
<e>\one\approx\sum_i \ket{{\bf r}_i}\bra{{\bf r}_i}</e> 
in the case of a grid (or some other basis like plane waves can be used instead
of <m>\ket{{\bf r}_i}</m>) and <m>n({\bf r})=\braket{{\bf r}|n}</m> is approximated by
<m>n({\bf r}_i)=\braket{{\bf r}_i|n}</m>. 
Let 
<e>\x=(x_1,x_2,x_3,...),\quad x_i\equiv n({\bf r}_i)=\braket{{\bf r}_i|n}</e>
and
<e>\F(\x_M)\equiv \hat F n_M, \quad F_i= (\hat F n_M)({\bf r}_i)</e>
the self-consistency is reached when <m>\F(\x)=\x</m>.
</p>
<p>
So the problem is in solving the equation
<e>\F(\x)=\x</e>
where <m>\x</m> denotes a vector in many dimensions (the number of points in the
grid). It can also be expressed in the
form of the residual <m>\R(\x)=\F(\x)-\x</m> as
<e>\R(\x)=0</e>
Almost all of the methods start with approximating
<e id="lin">\R(\x_{M+1})-\R(\x_M)\approx\J\cdot(\x_{M+1}-\x_M)</e>
where the Jacobian
<e>J_{ij}={\partial R_i\over\partial x_j}</e>
We want <m>\R(\x_{M+1})=0</m>, so substituting that into <a ref="lin"/> we
get
<e>\x_{M+1}\approx\x_M+\J^{-1}\cdot(\R(\x_{M+1})-\R(\x_M))=
\x_M-\J^{-1}\cdot\R(\x_M)</e>
If we knew the Jacobian exactly, this would be the multidimensional
Newton-Raphson method, but we can only make approximations to <m>\J</m> using a
sequence of <m>\J_0</m>, <m>\J_1</m>, <m>\J_2</m>, \dots: 
<e id="Jaciter">\x_{M+1}=\x_M-\J_M^{-1}\cdot\R(\x_M)</e>
and the rate of convergence is determined by the quality of the Jacobian. These type of methods are called quasi-Newton-Raphson methods. 
</p>
<p>
    The simplest approach is to use the <em>linear mixing</em> scheme for which
<e>\J_M^{-1}=-\alpha\one</e>
so
<e>\x_{M+1}=\x_M+\alpha\R(\x_M)=\x_M+\alpha(\F(\x_M)-\x_M)</e>
where <m>0&lt;\alpha\le1</m> is the mixing parameter, working value is somewhere
around <m>\alpha=0.1</m> to <m>\alpha=0.3</m>.
Unfortunately, this procedure is slow and also we do not explore all 
the possible densities with this mixing, which means that we don't
get the correct density with any accuracy, because we get stuck at a "stiff"
situation for which continued iteration does not improve the distance 
<m>|\R(\x_M)|</m> between input and output densities. On the other hand it's very
easy to implement and it works in most cases, although slowly.
</p>
<p>
Surprisingly very good method is this:
<e>\J_M^{-1}=-{\rm diag}(\beta_1,\beta_2,\beta_3,\dots)</e>
start with <m>\beta_1=\beta_2=\beta_3=\cdots=\alpha</m> and
at every iteration adjust the parameters <m>\beta_i</m> according to this very
simple algorithm: if <m>R_i(\x_{M-1})R_i(\x_M)>0</m> then increase <m>\beta_i</m> by
<m>\alpha</m> (if <m>\beta_i>\alpha_{max}</m>, set <m>\beta_i=\alpha_{max}</m>) otherwise set
<m>\beta_i=\alpha</m>. In my tests it behaves almost as well as the second Broyden
method.
</p>
<p>
More sophisticated approach is the Broyden update, which 
updates the <m>\J</m> successively at every iteration. The <em>first Broyden
    method</em> is using this formula: 
<e>\J_{M+1}=\J_M-{(\Delta\R(\x_M)+\J_M\cdot\Delta\x_M)\Delta\x_M^T\over 
|\Delta\x_M|^2}</e>
which has the disadvantage that we need to compute the inverse Jacobian in
<a ref="Jaciter"/> at every iteration, which is impossible in our case. The
<em>it
    second Broyden method</em> updates the inverse Jacobian directly using this
formula
<e id="Bupdate">\J_{M+1}^{-1}=\J_M^{-1}+{(\Delta\x_M-\J_M^{-1}\cdot\Delta\R(\x_M))
\Delta\R(\x_M)^T\over |\Delta\R(\x_M)|^2}</e>
starting with the linear mixing:
<e>\J_0^{-1}=-\alpha\one</e>
It is impossible to store the whole dense matrix of the inverse Jacobian, but
fortunately it is not necessary, realising that the <a ref="Bupdate"/> has a very
simple structure \cite{srivastava}:
<e>\J_{M+1}^{-1}=\J_M^{-1}+{\bf u}{\bf v}^T</e> 
with
<e id="vecu">{\bf u}=\Delta\x_M-\J_M^{-1}\cdot\Delta\R(\x_M)</e>
<e>{\bf v}={\Delta\R(\x_M)\over |\Delta\R(\x_M)|^2}</e>
so the whole inverse Jacobian can be written as
<e>\J_M^{-1}=-\alpha\one+{\bf u}_1{\bf v}_1^T+{\bf u}_2{\bf v}_2^T+
{\bf u}_3{\bf v}_3^T+\cdots</e>
and we only need to know how to apply such a Jacobian to an arbitrary vector,
which is needed in <a ref="vecu"/> and <a ref="Jaciter"/>:
<e>\J_M^{-1}\cdot\y=-\alpha\y+{\bf u}_1({\bf v}_1^T\y)
+{\bf u}_2({\bf v}_2^T\y)+ {\bf u}_3({\bf v}_3^T\y)+\cdots</e>
Thus instead of the whole dense matrix, we only need to save the vectors <m>{\bf
u}</m> and <m>{\bf v}</m> from every iteration.
</p>
<p>
    Vanderbilt and Louie \cite{vanderbiltlouie} suggested a <em>modified Broyden
        method</em>, which incorporates weights, but Eyert \cite{eyert} showed that if all
the weights are used to tune the iteration process to its fastest convergence,
they, in fact, cancel out and the result of the scheme is called by Eyert the
<em>generalized Broyden method</em>, whose scheme shown by Eyert is exactly the
same as for the <em>Anderson mixing</em>:
<e>\sum_{p=M-k}^{M-1}(1+\omega_0^2\delta_{pn})\Delta\R(\x_n)^T\Delta\R(\x_p)
\gamma_p =\Delta\R(\x_n)^T\R(\x_M) </e>
<e>\x_{M+1}=\x_M+\beta_M\R(\x_M)-\sum_{p=M-k}^{M-1}\gamma_p
(\Delta\x_p+\beta_M\Delta\R(\x_p)) </e>
which according to Eyert should converge even faster than the second Broyden
method, but it doesn't in my own implementation. <m>\omega_0</m> is added just for a
numerical stability, good value is <m>\omega=0.01</m>, but it can also be switched
off by <m>\omega_0=0</m>. <m>p</m> is the number of last iterations to use, good value
according to Eyert is <m>p=5</m>, <m>\beta_M</m> shouldn't influence the convergence
much for <m>p=5</m>.
</p>
<p>
The problem with <m>n</m> is that there are two conditions which need to be
satisfied
<e>n>0</e>
and the normalization
<e>\int n({\bf r}) \d^3r=Z</e>
The Newton method converges to the correct norm, but slowly. The condition
<m>n>0</m> however causes great instability. One option could be to use a logistic
function like
<e>n(r)={C\over 1+e^{-x(r)}}</e>
for sufficiently large <m>C</m> and solve for <m>x</m>, which can be both positive and
negative. But more elegant solution is to mix <m>V_h+V_{xc}</m> instead of the
densities.
</p>
</section>
<section>
    <title>Example: Pb Atom</title>
    <p>
To illustrate the explained theory, we will show how to calculate the Pb atom.
We have <m>N=82</m> and
<e>v({\bf r}_i)=-{82\over |{\bf r}_i|}</e>
and we need to sum over the lowest 82 eigenvalues in <a ref="KSdensity"/>. One
option (the correct one) is to automatically try different "n" and "l" until we
are sure we got the lowest 82 energies. But for Pb the combination of "n" and
"l" is well-known, it is
(first number is
<m>n</m>, the letter is <m>l</m> and the number in superscript gives the number of
times the particular eigenvalue needs to be taken into account in the sum):
<m>1S^2</m>, <m>2S^2</m>, <m>2P^6</m>, <m>3S^2</m>, <m>3P^6</m>, <m>3D^{10}</m>, <m>4S^2</m>, <m>4P^6</m>, <m>4D^{10}</m>,
<m>4F^{14}</m>, <m>5S^2</m>, <m>5P^6</m>, <m>5D^{10}</m>, <m>6S^2</m>, <m>6P^2</m>
(notice the 5F and 5G are missing). Together it is 82 eigenvalues. 
The KS energies for these eigenvalues are:
</p>
<p>
-2701.6    -466.18    -471.87    -111.45    -108.24    -92.183
-24.498    -22.086    -15.119    -5.6606    -3.9570
 -2.9743    -.92718    -.33665    -.14914
 </p>
    </section>
    </section>
