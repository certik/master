\section{Radial Schrödinger and Dirac Equations}

For general treatment, together with derivation of all the different forms of
radial Dirac equations used in the literature, see the author's bachelor thesis
\cite{bachelor-thesis}. Here we just summarize the results.

\subsection{Radial Schrödinger Equation}

We have a spherically symmetric potential energy 
\begin{equation*}
  V({\bf x})=V(r)\,.
\end{equation*}
State with a given square of an angular momentum (eigenvalue $l(l+1)$) and its $z$ component (eigenvalue $m$) is described by the wave function 
\begin{equation}
  \psi_{nlm}({\bf x})=R_{nl}(r)\,Y_{lm}\left({\bf x}\over r\right)\,,  \label{psi}
\end{equation}
where $R_{nl}(r)$ obeys the equation \cite{formanek} (eq. 2.400) 
\begin{equation}
  R_{nl}''+{2\over r}R_{nl}'+{2M\over\hbar^2}(E-V)R_{nl}- {l(l+1)\over r^2}R_{nl}=0\,.  \label{radial}
\end{equation}
This is called the radial Schrödinger equation which we want to solve numerically.

\subsection{Numerical integration for a given }

Equation (\ref{radial}) is the linear ordinary differential equation of the second order, so the general solution is a linear combination of two independent solutions. Normally, the $2$ constants are determined from initial and/or boundary conditions. In our case, however, we don't have any other condition besides being interested in solutions that we can integrate on the interval $(0,\infty)$ (and which are normalizable), more exactly we want $R\in L^2$ and $\int_0^\infty r^2R^2\,\d r=1$.

It can be easily shown by a direct substitution, that there are only two asymptotic behaviors near the origin: $r^l$ and $r^{-l-1}$. We are interested in quadratic integrable solutions only, so we are left with $r^l$ and only one integration constant, which we calculate from a normalization. This determines the solution uniquely.

All the integration algorithms needs to evaluate $R''$, which is a problem at the origin, where all the terms in the equation are infinite, although their sum is finite. We thus start to integrate the equation at some small $r_0$ (for example $r_0=10^{-10}\rm\,a.u.$), where all the terms in the equation are finite. If we find the initial conditions $R(r_0)$ and $R'(r_0)$, the solution is then fully determined.

If $r_0$ is sufficiently small, we can set $R(r_0)=r_0^l$ and $R'(r_0)=lr_0^{l-1}$. In the case $l=0$ we need to set $R(r_0)=1$ and $R'(r_0)=-{1\over a}$, where $a$ is the Bohr radius, see the next section for more details.

So when somebody gives us $l$ and $E$, we are now able to compute the solution up to the the multiplicative constant that is later determined from a normalization. As was already mentioned, we used the fourth-order Runge-Kutta method that proved very suitable for this problem.

\subsection{Asymptotic behavior}

The asymptotic behavior is important for the integration routine to find the correct solution for a given $E$. It is well known, that the first term of the Taylor series of the solution is $r^l$, independent of the potential \cite{formanek} (eq. 2.408). This is enough information to find the correct solution for $l>0$ because the only thing we need to know is the value of the wave function and its derivative near the origin, which is effectively $r_0^l$ and $lr_0^{l-1}$ for some small $r_0$. The problem is with $l=0$, where the derivative cannot be calculated just from $l$ and $r_0$.

The asymptotic behavior for $l=0$ depends on the potential $V$, so we need to take into account it's properties. We assume $V$ to be of a form: 
\begin{equation*}
  V=-{Z\over r}+v_0 + v_1r + O(r^2)\,,
\end{equation*}
It can be shown, that the solution is then 
\begin{equation*}
  R=a_0(1-{r\over a}+O(r^2))\,,
\end{equation*}
where $a={\hbar^2\over ZM}$ is the Bohr radius and $a_0$ is a normalization constant. So the initial condition for the integration for $l=0$ is $R(r_0)=1$ and $R'(r_0)=-{1\over a}$.

\subsection{Dirac Equation}

The Dirac equation for one particle is \cite{strange,zabloudil}: 
\begin{equation}
  H\psi=W\psi\,,  \label{diraceq}
\end{equation}
\begin{equation*}
  H=c\balpha\cdot{\bf p}+\beta mc^2+V(r)\hbox{\dsrom1}\,,
\end{equation*}
where $\psi$ is a four component vector: 
\begin{equation*}
  \psi=\left(\matrix{\psi_1\psi_2\psi_3\psi_4}\right) =\col{\psi_A}{\psi_B}\,,\qquad \psi_A=\col{\psi_1}{\psi_2}\!,\,\psi_B=\col{\psi_3}{\psi_4}
\end{equation*}
and $\balpha$, $\beta$ are $4\times4$ matrices: 
\begin{equation*}
  \balpha=\mat{0}{\bsigma}{\bsigma}{0}\,,
\end{equation*}
\begin{equation*}
  \beta=\mat{\hbox{\dsrom1}}{0}{0}{-\hbox{\dsrom1}}\,,
\end{equation*}
where the Pauli matrices $\bsigma=(\sigma_x,\sigma_y,\sigma_z)$ and $\hbox{\dsrom1}$ form a basis of all $2\times2$ Hermitian matrices. To derive a continuity equation, we multiply (\ref{diraceq}) by $\psi^*$ and subtract the conjugate transpose of (\ref{diraceq}) multiplied by $\psi$: 
\begin{equation*}
  \p{}{t}(\psi^*\psi)=-\nabla\cdot(c\psi^*\balpha\psi)\,,
\end{equation*}
so we identify the probability and current densities as 
\begin{equation*}
  \rho=\psi^*\psi=\psi_1^*\psi_1+\psi_2^*\psi_2+\psi_3^*\psi_3+\psi_4^*\psi_4\,, \qquad {\bf j}=c\psi^*\balpha\psi\,.
\end{equation*}
The normalization of a four-component wave function is then 
\begin{equation}
  \int \rho \,\d^3x= \int \psi^*\psi \,\d^3x= \int \psi_1^*\psi_1+\psi_2^*\psi_2+\psi_3^*\psi_3+\psi_4^*\psi_4 \,\d^3x= 1\,.  \label{norm}
\end{equation}
The probability density $\rho(x,y,z)$ is the physical quantity we are interested in, and all the four-component wavefunctions and other formalism is just a way of calculating it. This $\rho$ is also the thing we should compare with the Schrödinger equation.

\subsection{Radial Dirac equation}

We and search for a basis in the form of spin angular functions: 
\begin{equation}
  \psi_A=g\chi^{j_3}_{\kappa}\,,  \label{psia}
\end{equation}
\begin{equation}
  \psi_B=if\chi^{j_3}_{-\kappa}\,.  \label{psib}
\end{equation}
Substituting all of these into (\ref{diraceq}) and some more well-known manipulations one gets: 
\begin{equation}
  \hbar c \col{-\p{f}{r}+{\kappa-1\over r}f} {\p{g}{r}+{\kappa+1\over r}g}= \col{(W-V-mc^2)g}{(W-V+mc^2)f}\,.  \label{radialdirac}
\end{equation}
This is the radial Dirac equation. As we shall see in the next section, the equation for $g$ is (with the exception of a few relativistic corrections) identical to the radial Schrödinger equation. And $f$ vanishes in the limit $c\to\infty$. For this reason $f$ is called the small (fein, minor) component and $g$ the large (groß, major) component.

The probability density is 
\begin{equation*}
  \rho=\psi^*\psi=\psi^*_A\psi_A+\psi^*_B\psi_B= f^2\chi^{j_3*}_{-\kappa}\chi^{j_3}_{-\kappa}+ g^2\chi^{j_3*}_{\kappa}\chi^{j_3}_{\kappa}\,,
\end{equation*}
so from the normalization condition (\ref{norm}) we get 
\begin{equation*}
  \int \rho \,\d^3x= \int f^2\chi^{j_3*}_{-\kappa}\chi^{j_3}_{-\kappa}+ g^2\chi^{j_3*}_{\kappa}\chi^{j_3}_{\kappa} \,\d^3x= \int (f^2\chi^{j_3*}_{-\kappa}\chi^{j_3}_{-\kappa}+ g^2\chi^{j_3*}_{\kappa}\chi^{j_3}_{\kappa})\,r^2\,\d r\d\Omega=
\end{equation*}
\begin{equation*}
  =\int_0^\infty f^2r^2\,\d r\int\chi^{j_3*}_{-\kappa}\chi^{j_3}_{-\kappa} \,\d\Omega+ \int_0^\infty g^2r^2\,\d r\int\chi^{j_3*}_{\kappa}\chi^{j_3}_{\kappa} \,\d\Omega= \int_0^\infty r^2(f^2+g^2)\,\d r=1\,,
\end{equation*}
where we used the normalization of spin-angular functions. Also it can be seen, that the radial probability density is 
\begin{equation}
  \rho(r)=r^2(f^2+g^2)  \label{radialrho}
\end{equation}
(i.e., the probability to find the electron between $r_1$ and $r_2$ is $\int_{r_1}^{r_2}r^2(f^2+g^2)\,\d r$). The result of integrating the radial Dirac equation are the two functions $f$ and $g$, but the physically relevant quantity is the radial probability density (\ref{radialrho}). In the nonrelativistic case, the density is given by 
\begin{equation*}
  \rho(r)=r^2R^2\,,
\end{equation*}
so the correspondence between the Schrödinger and Dirac equation is $R^2=f^2+g^2$.

For numerical stability and robustness, we are not solving the equations in the form (\ref{radialdirac}), but a sligthly rearranged ones. Let's use Hartree atomic units ($m=\hbar=1$) and define $E=W-mc^2=W-c^2$, so that $E$ doesn't contain the electron rest mass energy. Let's make the substitution \cite{donald:apw} 
\begin{equation*}
  P_\kappa=rg_\kappa\,,
\end{equation*}
\begin{equation*}
  Q_\kappa=rf_\kappa
\end{equation*}
and plug all of this into (\ref{radialdirac}). After a little manipulation we get: 
\begin{equation*}
  {\d P_\kappa\over\d r}=-{\kappa\over r}P_\kappa+\left[{E-V\over c}+2c\right]Q_k\,,
\end{equation*}
\begin{equation}
  {\d Q_\kappa\over\d r}={\kappa\over r}Q_\kappa-{1\over c}(E-V)P_k\,,  \label{radialdirac2}
\end{equation}
which can be found in \cite{zabloudil} (eq. 8.12 and 8.13), where they have one $c$ hidden in $Q_\kappa=crf_\kappa$ and use Rydberg atomic units, so they have $1$ instead of $2$ in the square bracket. It can be found in \cite{bachelet} as well, they use Hartree atomic units, but have a different notation $G_\kappa\equiv P_\kappa$ and $F_\kappa\equiv Q_\kappa$, also they made a substitution $c={1\over\alpha}$.

\subsection{Asymptotic behavior}

We calculate the functions $f_\kappa$ and $g_\kappa$ in a similar way as we calculated $R$ for the Schrödinger equation, thus we need the asymptotic behavior at the origin. The potential can always be treated as $V=1/r+\cdots$ and in this case it can be shown \cite{zabloudil}, that the asymptotic is 
\begin{equation*}
  P_\kappa = r g_\kappa=r^{\beta}\,,
\end{equation*}
\begin{equation*}
  Q_\kappa = r f_\kappa=r^{\beta-1}{\beta+\kappa\over{E-V\over c}+2c}\,,
\end{equation*}
where 
\begin{equation}
  \beta=\sqrt{\kappa^2-\left(Z\over c\right)^2}\,,  \label{diracasymptotic}
\end{equation}
or, if we write it explicitly, for $j=l+\half$
\begin{equation*}
  \beta^+=\sqrt{(-l-1)^2-\left(Z\over c\right)^2}
\end{equation*}
and $j=l-\half$
\begin{equation*}
  \beta^-=\sqrt{l^2-\left(Z\over c\right)^2}\,.
\end{equation*}
In the semirelativistic case (which is an approximation --- we neglect the spin-orbit coupling term) we choose 
\begin{equation*}
  \beta=\sqrt{\half(|\beta^+|^2+|\beta^-|^2)}= \sqrt{l^2+l+\half-\left(Z\over c\right)^2}\,.
\end{equation*}
It should be noted that in the literature we can find other types of aymptotic behaviour for the semirelativistic case, its just a question of the used approximation. One can hardly say that some of them are correct and another is not since the semirelativistic (sometimes denoted as scalar-relativstic) approximation itself is not correct, it's just an approximation.

It follows from (\ref{diracasymptotic}) that for $j=l+\half$ the radial Dirac equation completely becomes the radial Schrödinger equation in the limit $c\to\infty$ (and gives exactly the same solutions): 
\begin{equation*}
  P_\kappa = r g_\kappa \to r^{l+1}\,,
\end{equation*}
\begin{equation*}
  Q_\kappa = r f_\kappa \to 0\,.
\end{equation*}
For $j=l-\half$ however, we get a wrong asymptotic: we get a radial Schrödinger equation for $l$, but the asymptotic for $l-1$.

\subsection{Eigenproblem}

In the previous sections, we learned how to calculate the solution of both the radial Schrödinger and Dirac equations for a given $E$. For most of the energies, however, the solution for $r\to\infty$ exponentially diverges to $\pm\infty$. Only for the energies equal to eigenvalues, the solution tends exponentially to zero for $r\to\infty$. The spectrum for bounded states is discrete, so we label the energies by $n$, starting from $1$.

We want to find the eigenvalue and eigenfunction for a given $n$ and $l$ (and a spin in the relativistic case). The algorithm is the same for both nonrelativistic and relativistic case and is based on two facts, first that the number of nodes (ie. the number of intersections with the $x$ axis, not counting the one at the origin and in the infinity) of $R_{nl}$ and $g_\kappa$ is $n-l-1$ and second that the solution must tend to zero at infinity.

We calculate the solution for some (random) energy $E_0$, using the procedure described above. Then we count the number of nodes (for diverging solutions, we don't count the last one) and check, if the solution is approaching the zero from top or bottom in the infinity. From the number of nodes and the direction it is approaching the zero it can be determined whether the energy $E_0$ is below or above the eigenvalue $E$ belonging to a given $n$ and $l$. The rest is simple, we find two energies, one below $E$, one above $E$ and by bisecting the interval we calculate $E$ with any precision we want.

There are a few technical numerical problems that are unimportant from the theoretical point of view, but that need to be solved if one attempts to actually implement this algorithm. One of them is that when the algorithm (described in the previous paragraph) finishes, because the energy interval is sufficiently small, it doesn't mean the solution is near zero for the biggest $r$ of our grid. Remember, the solution goes exponentially to $\pm\infty$ for every $E$ except the eigenvalues and because we never find the exact eigenvalue, the solution will (at some point) diverge from zero.

Possible solution that we have employed is as follows: when the algorithm finishes we find the last minimum (which is always near zero) and trim the solution behind it (set it to zero).

The second rather technical problem is how to choose the initial interval of energies so that the eigenvalue lies inside the interval. We use some default values that work for atomic calculations, while allowing the user to override it if needed.


\section{Density Functional Theory}

\subsection{Introduction}

Good books about DFT are \cite{DFT} and \cite{martin}, but they both contain much more topics which we don't need and some topics are missing in each of them, so this chapter gives a self-contained explanation of all one has to know about a many body quantum mechanics and the DFT in order to be able to do DFT calculations.

\subsection{Many Body Schrödinger Equation}

We use the Born-Oppenheimer approximation, which says that the nuclei of the treated atoms are seen as fixed. A stationary electronic state (for $N$ electrons) is then described by a wave function $\Psi({\bf r_1},{\bf r_2},\cdots,{\bf r}_N)$ fulfilling the many-body Schrödinger equation 
\begin{equation*}
  \hat H\ket\Psi=(\hat T+\hat U+\hat V)\ket\Psi=E\ket\Psi
\end{equation*}
where 
\begin{equation*}
  \hat T = \sum_i^N -{1\over2}\nabla_i^2
\end{equation*}
is the kinetic term, 
\begin{equation*}
  \hat U = \sum_{i<j}U({\bf r_i},{\bf r_j})= {1\over2}\sum_{i,j}U({\bf r_i},{\bf r_j})
\end{equation*}
\begin{equation*}
  U({\bf r_i},{\bf r_j})=U({\bf r_j},{\bf r_i})={1\over|{\bf r_i}-{\bf r_j}|}
\end{equation*}
is the electron-electron interaction term and 
\begin{equation*}
  \hat V = \sum_i^N v({\bf r_i})
\end{equation*}
\begin{equation*}
  v({\bf r_i})=\sum_k -{Z_k\over|{\bf r_i}-{\bf R_k}|}
\end{equation*}
is the interaction term between electrons and nuclei, where $R_k$ are positions of nuclei and $Z_k$ the number of nucleons in each nucleus (we are using atomic units). So for one atomic calculation with the atom nucleus in the origin, we have just $v({\bf r_i})=-{Z\over|{\bf r_i}|}$.

$|\Psi|^2=\Psi^*\Psi$ gives the probability density of measuring the first electron at the position $\bf r_1$, the second at $\bf r_2$, \dots and the Nth electron at the position ${\bf r}_N$. The normalization is such that $\int |\Phi|^2\d^3 r_1\d^3 r_2\dots\d^3 r_N=1$. The $\Psi$ is antisymmetric, i.e. $\Psi({\bf r_1},{\bf r_2},\cdots,{\bf r}_N)= -\Psi({\bf r_2},{\bf r_1},\cdots,{\bf r}_N)= -\Psi({\bf r_1},{\bf r}_N,\cdots,{\bf r_2})$ etc.

Integrating $|\Psi|^2$ over the first $N-1$ electrons is the probability density that the $N$th electron is at the position ${\bf r}_N$. Thus the probability density $n({\bf r})$ that any of the N electrons (i.e the first, or the second, or the third, \dots, or the $N$th) is at the position $\bf r$ is called the particle (or charge or electron) density and is therefore given by: 
\begin{equation*}
  n({\bf r})= \int \Psi^*({\bf r},{\bf r}_2,\cdots,{\bf r}_N) \Psi ({\bf r},{\bf r}_2,\cdots,{\bf r}_N) \,\d^3 r_2\,\d^3 r_3\cdots\d^3 r_N+
\end{equation*}
\begin{equation*}
  +\int \Psi^*({\bf r}_1,{\bf r},\cdots,{\bf r}_N) \Psi ({\bf r}_1,{\bf r},\cdots,{\bf r}_N) \,\d^3 r_1\,\d^3 r_3\cdots\d^3 r_N+\cdots
\end{equation*}
\begin{equation*}
  +\int \Psi^*({\bf r}_1,{\bf r}_2,\cdots,{\bf r}) \Psi ({\bf r}_1,{\bf r}_2,\cdots,{\bf r}) \,\d^3 r_1\,\d^3 r_2\,\d^3 r_3\cdots\d^3 r_{N-1}=
\end{equation*}
\begin{equation*}
  =\int(\delta({\bf r}-{\bf r}_1)+\delta({\bf r}-{\bf r}_2)+\cdots+\delta({\bf r}-{\bf r}_N))
\end{equation*}
\begin{equation*}
  \Psi^*({\bf r}_1,{\bf r}_2,\cdots,{\bf r}_N) \Psi ({\bf r}_1,{\bf r}_2,\cdots,{\bf r}_N) \,\d^3 r_1\,\d^3 r_2\,\d^3 r_3\cdots\d^3 r_{N}=
\end{equation*}
\begin{equation*}
  =\sum_{i=1}^N\int \braket{\Psi|{\bf r}_1,{\bf r}_2,\cdots,{\bf r}_N}\delta({\bf r}-{\bf r}_i) \braket{{\bf r}_1,{\bf r}_2,\cdots,{\bf r}_N|\Psi} \,\d^3 r_1\,\d^3 r_2\,\d^3 r_3\cdots\d^3 r_{N}=
\end{equation*}
\begin{equation*}
  =N\int \braket{\Psi|{\bf r}_1,{\bf r}_2,\cdots,{\bf r}_N}\delta({\bf r}-{\bf r}_1) \braket{{\bf r}_1,{\bf r}_2,\cdots,{\bf r}_N|\Psi} \,\d^3 r_1\,\d^3 r_2\,\d^3 r_3\cdots\d^3 r_{N}=
\end{equation*}
\begin{equation}
  =N\int \Psi^*({\bf r},{\bf r}_2,\cdots,{\bf r}_N) \Psi ({\bf r},{\bf r}_2,\cdots,{\bf r}_N) \,\d^3 r_2\,\d^3 r_3\cdots\d^3 r_N  \label{chargedensity}
\end{equation}
Thus $\int_\Omega n({\bf r})\,\d^3r$ gives the number of particles (and also the amount of charge) in the region of integration $\Omega$. Obviously $\int n({\bf r})\,\d^3r=N$.

The energy of the system is given by 
\begin{equation}
  E=\braket{\Psi|\hat H|\Psi}= \braket{\Psi|\hat T|\Psi}+\braket{\Psi|\hat U|\Psi}+\braket{\Psi|\hat V|\Psi}= T+U+V  \label{Emanybody}
\end{equation}
where 
\begin{equation*}
  T=\braket{\Psi|\hat T|\Psi}=\sum_i^N\int \Psi^*({\bf r_1},{\bf r_2},\cdots,{\bf r_N})(-\half\nabla_i^2) \Psi({\bf r_1},{\bf r_2},\cdots,{\bf r_N})\,\d^3 r_1\,\d^3 r_2\cdots\d^3 r_N
\end{equation*}
\begin{equation*}
  U=\braket{\Psi|\hat U|\Psi}
\end{equation*}
\begin{equation*}
  V=\braket{\Psi|\hat V|\Psi}=\sum_i^N\int \Psi^*({\bf r_1},{\bf r_2},\cdots,{\bf r_N})v({\bf r_i}) \Psi({\bf r_1},{\bf r_2},\cdots,{\bf r_N})\,\d^3 r_1\,\d^3 r_2\cdots\d^3 r_N=
\end{equation*}
\begin{equation*}
  =\sum_i^N\int \Psi^*({\bf r_1},{\bf r_2},\cdots,{\bf r_N})v({\bf r_1}) \Psi({\bf r_1},{\bf r_2},\cdots,{\bf r_N})\,\d^3 r_1\,\d^3 r_2\cdots\d^3 r_N=
\end{equation*}
\begin{equation*}
  =N\int \Psi^*({\bf r_1},{\bf r_2},\cdots,{\bf r_N})v({\bf r_1}) \Psi({\bf r_1},{\bf r_2},\cdots,{\bf r_N})\,\d^3 r_1\,\d^3 r_2\cdots\d^3 r_N=
\end{equation*}
\begin{equation}
  =\int v({\bf r}) n({\bf r})\d^3 r=V[n]  \label{V[n]}
\end{equation}
It needs to be stressed, that $E$ generally is \textbf{not} a functional of $n$ alone, only the $V[n]$ is. In the next section we show however, that if the $\ket{\Psi}$ is a ground state (of any system), then $E$ becomes a functional of $n$.

\subsection{The Hohenberg-Kohn Theorem}

The Schrödinger equation gives the map
\begin{equation*}
  C: V \to \Psi
\end{equation*}
where $\Psi$ is the ground state. C is bijective (one-to-one correspondence),
because to every $V$ we can compute the corresponding $\Psi$ from Schrödinger
equation and two
different $V$ and $V'$ (differing by more than a constant) give two different
$\Psi$, because if $V$ and $V'$ gave the same $\Psi$, then by substracting 
\begin{equation*}
  \hat H\ket{\Psi}=E_{gs}\ket{\Psi}
\end{equation*}
from 
\begin{equation*}
  \hat H'\ket{\Psi}=(\hat H-\hat V+\hat V')\ket{\Psi}=E_{gs}'\ket{\Psi}
\end{equation*}
we would get $V-V'=E-E'$, which is a contradiction with the assumption that $V$ and $V'$ differ by more than a constant.

Similarly, from the ground state wavefunction $\Psi$ we can compute the charge density $n$ giving rise to the map 
\begin{equation*}
  D: \Psi \to n
\end{equation*}
which is also bijective, because to every $\Psi$ we can compute $n$ from (\ref{chargedensity}) and two different $\Psi$ and $\Psi'$ give two different $n$ and $n'$, because different $\Psi$ and $\Psi'$ give 
\begin{equation*}
  E_{gs}=\braket{\Psi|\hat H|\Psi}<\braket{\Psi'|\hat H|\Psi'}= \braket{\Psi'|\hat H'+\hat V-\hat V'|\Psi'}=E_{gs}'+\int n'({\bf r}) (v({\bf r})-v'({\bf r}))\,\d^3 r
\end{equation*}
\begin{equation*}
  E_{gs}'=\braket{\Psi'|\hat H'|\Psi'}<\braket{\Psi|\hat H'|\Psi}= \braket{\Psi|\hat H+\hat V'-\hat V|\Psi}=E_{gs}+\int n({\bf r}) (v'({\bf r})-v({\bf r}))\,\d^3 r
\end{equation*}
adding these two inequalities together gives 
\begin{equation*}
  0<\int n'({\bf r}) (v({\bf r})-v'({\bf r}))\,\d^3 r + \int n({\bf r}) (v'({\bf r})-v({\bf r}))\,\d^3 r= \int (n({\bf r})-n'({\bf r}))(v'({\bf r})-v({\bf r}))\,\d^3 r
\end{equation*}
which for $n=n'$ gives $0<0$, which is nonsense, so $n\neq n'$.

So we have proved that for a given ground state density $n_0({\bf r})$ (generated by a potential $\hat V_0$) it is possible to calculate the corresponding ground state wavefunction $\Psi_0({\bf r_1},{\bf r_2},\cdots,{\bf r_N})$, in other words, $\Psi_0$ is a unique functional of $n_0$: 
\begin{equation*}
  \Psi_0=\Psi_0[n_0]
\end{equation*}
so the ground state energy $E_0$ is also a functional of $n_0$
\begin{equation*}
  E_0=\braket{\Psi_0[n_0]|\hat T+\hat U+\hat V_0|\Psi_0[n_0]}=E[n_0]
\end{equation*}
We define an energy functional 
\begin{equation}
  E_{v_0}[n]=\braket{\Psi[n]|\hat T+\hat U+\hat V_0|\Psi[n]}= \braket{\Psi[n]|\hat T+\hat U|\Psi[n]}+\int v_0({\bf r})n({\bf r})\d^3r  \label{Efunct}
\end{equation}
where $\ket{\Psi[n]}$ is any ground state wavefunction (generated by an arbitrary potential), that is, $n$ is a ground state density belonging to an arbitrary system. $E_0$ which is generated by the potential $V_0$ can then be expressed as 
\begin{equation*}
  E_0=E_{v_0}[n_0]
\end{equation*}
and for $n\neq n_0$ we have (from the Ritz principle) 
\begin{equation*}
  E_0<E_{v_0}[n]
\end{equation*}
and one has to minimise the functional $E_{v_0}[n]$: 
\begin{equation}
  E_0=\min_n E_{v_0}[n]  \label{Emin}
\end{equation}
The term 
\begin{equation*}
  \braket{\Psi[n]|\hat T+\hat U|\Psi[n]}\equiv F[n]
\end{equation*}
in (\ref{Efunct}) is universal in the sense that it doesn't depend on $\hat V_0$. It can be proven \cite{DFT}, that $F[n]$ is a functional of $n$ for degenerated ground states too, so (\ref{Emin}) stays true as well.

The ground state densities in (\ref{Efunct}) and (\ref{Emin}) are called \textbf{pure-state v-representable} because they are the densities of (possible degenerate) ground state of the Hamiltonian with some local potential $v({\bf r})$. One may ask a question if all possible functions are v-representable (this is called the v-representability problem). The question is relevant, because we need to know which functions to take into account in the minimisation proccess (\ref{Emin}). Even though not every function is v-representable \cite{DFT}, every density defined on a grid (finite of infinite) which is strictly positive, normalized and consistent with the Pauli principle is ensemble v-representable. Ensemble v-representation is just a simple generalization of the above, for details see \cite{DFT}. In plain words, we are fine.

The functional $E_{v_0}[n]$ in (\ref{Emin}) depends on the particle number $N$, so in order to get $n$, we need to solve the variational formulation 
\begin{equation*}
  {\delta\over\delta n}\left(E_v[n]-\mu(N)\int n(\bf r)\d^3r\right)=0
\end{equation*}
so 
\begin{equation}
  {\delta E_v[n]\over\delta n}=\mu(N)  \label{euler}
\end{equation}
Let the $n_N(\bf r)$ be the solution of (\ref{euler}) with a particle number $N$ and the energy $E_N$: 
\begin{equation*}
  E_N=E_v[n_N]
\end{equation*}
The Lagrangian multiplier $\mu$ is the exact chemical potential of the system 
\begin{equation*}
  \mu(N)={\partial E_N\over\partial N}
\end{equation*}
becuase 
\begin{equation*}
  E_{N+\epsilon}-E_N=E_v[n_{N+\epsilon}]-E_v[n_N] =\int {\delta E_v\over\delta n} (n_{N+\epsilon}-n_N)\d^3r=
\end{equation*}
\begin{equation*}
  =\int \mu(N) (n_{N+\epsilon}-n_N)\d^3r =\mu(N)(N+\epsilon-N)=\mu(N)\epsilon
\end{equation*}
so 
\begin{equation*}
  \mu(N)={E_{N+\epsilon}-E_N\over\epsilon} \ \longrightarrow \ {\partial E_N\over\partial N}
\end{equation*}

\subsection{The Kohn-Sham Equations}

Consider an auxiliary system of $N$ noninteracting electrons (noninteracting gas): 
\begin{equation*}
  \hat H_s=\hat T+\hat V_s
\end{equation*}
Then the many-body ground state wavefunction can be decomposed into single particle orbitals 
\begin{equation*}
  \ket{\Psi ({\bf r_1},{\bf r_2},\cdots,{\bf r_N})}= \ket{\psi_1({\bf r})}\ket{\psi_2({\bf r})}\cdots\ket{\psi_N({\bf r})}
\end{equation*}
and 
\begin{equation*}
  E_s[n]=T_s[\{\psi_i[n]\}]+V_s[n]
\end{equation*}
where 
\begin{equation*}
  T_s[n]=\braket{\Psi[n]|\hat T|\Psi[n]}= \sum_i\braket{\psi_i|-\half\nabla^2|\psi_i}
\end{equation*}
\begin{equation*}
  V_s[n]=\braket{\Psi[n]|\hat V|\Psi[n]}=\int v_s({\bf r})n({\bf r})\d^3r
\end{equation*}
From (\ref{euler}) we get 
\begin{equation}
  \mu={\delta E_s[n]\over\delta n({\bf r})}= {\delta T_s[n]\over\delta n({\bf r})}+{\delta V_s[n]\over\delta n({\bf r})}= {\delta T_s[n]\over\delta n({\bf r})}+v_s({\bf r})  \label{noninteract}
\end{equation}
Solution to this equation gives the density $n_s$.

Now we want to express the energy in (\ref{Emanybody}) using $T_s$ and $E_H$ for convenience, where $E_H$ is the classical electrostatic interaction energy of the charge distribution $n({\bf r})$: 
\begin{equation*}
  \nabla^2 V_H=n({\bf r})
\end{equation*}
or uquivalently 
\begin{equation*}
  E_H[n]=\half\int\int {n({\bf r})n({\bf r'})\over|{\bf r}-{\bf r'}|} \d^3r\d^3r'
\end{equation*}
\begin{equation}
  V_H({\bf r})={\delta E_H\over\delta n({\bf r})}=\half\int {n({\bf r'})\over|{\bf r}-{\bf r'}|} \d^3r'  \label{V_H}
\end{equation}
So from (\ref{Efunct}) we get 
\begin{equation*}
  E[n]=(T+U)[n]+V[n]=T_s[n]+E_H[n]+(T-T_s+U-E_H)[n]+V[n]=
\end{equation*}
\begin{equation}
  =T_s[n]+E_H[n]+E_{xc}[n]+V[n]  \label{Efunctxc}
\end{equation}
The rest of the energy is denoted by $E_{xc}=U-E_H+T-T_s$ and it is called is the exchange and correlation energy functional. From (\ref{euler})
\begin{equation*}
  \mu={\delta E[n]\over\delta n({\bf r})}= {\delta T_s[n]\over\delta n({\bf r})}+ {\delta E_H[n]\over\delta n({\bf r})}+ {\delta E_{xc}[n]\over\delta n({\bf r})}+ {\delta V[n]\over\delta n({\bf r})}
\end{equation*}
From (\ref{V_H}) we have 
\begin{equation*}
  {\delta E_H\over\delta n({\bf r})}=V_H({\bf r})
\end{equation*}
from (\ref{V[n]}) we get 
\begin{equation*}
  {\delta V[n]\over\delta n({\bf r})}=v({\bf r})
\end{equation*}
we define 
\begin{equation}
  {\delta E_{xc}[n]\over\delta n({\bf r})}=V_{xc}({\bf r})  \label{Vxcpot}
\end{equation}
so we arrive at 
\begin{equation}
  \mu={\delta E[n]\over\delta n({\bf r})}= {\delta T_s[n]\over\delta n({\bf r})}+V_H({\bf r})+V_{xc}({\bf r})+v({\bf r})  \label{interact}
\end{equation}
Solution to this equation gives the density $n$. Comparing (\ref{interact}) to (\ref{noninteract}) we see that if we choose 
\begin{equation*}
  v_s\equiv V_H+V_{xc}+v
\end{equation*}
then $n_s({\bf r})\equiv n({\bf r})$. So we solve the Kohn-Sham equations of this auxiliary non-interacting system 
\begin{equation}
  (-\half\nabla^2+v_s({\bf r}))\psi_i({\bf r}) \equiv(-\half\nabla^2+V_H({\bf r})+V_{xc}({\bf r})+v({\bf r}))\psi_i({\bf r}) =\epsilon_i\psi({\bf r})  \label{KSeq}
\end{equation}
which yield the orbitals $\psi_i$ that reproduce the density $n({\bf r})$ of the original interacting system 
\begin{equation}
  n({\bf r})\equiv n_s({\bf r})=\sum_i^N|\psi_i({\bf r})|^2  \label{KSdensity}
\end{equation}
The sum is taken over the lowest $N$ energies. Some of the $\psi_i$ can be degenerated, but it doesn't matter - the index $i$ counts every eigenfunction including all the degenerated. In plain words, the trick is in realising, that the ground state energy can be found by minimising the energy functional (\ref{Efunct}) and in rewriting this functional into the form (\ref{Efunctxc}), which shows that the interacting system can be treated as a noninteracting one with a special potential.

\subsection{The XC Term}

The exchange and correlation functional 
\begin{equation*}
  E_{xc}[n]=(T+U)[n]-E_H[n]-T_S[n]
\end{equation*}
can always be written in the form 
\begin{equation*}
  E_{xc}[n]=\int n({\bf r}')\epsilon_{xc}({\bf r}';n)\d^3r'
\end{equation*}
where the $\epsilon_{xc}({\bf r}';n)$ is called the xc energy density.

Unfortunately, no one knows $\epsilon_{xc}({\bf r}';n)$ exactly (yet). The most simple approximation is the \textbf{local density approximation} (LDA), for which the xc energy density $\epsilon_{xc}$ at $\bf r$ is taken as that of a homogeneous electron gas (the nuclei are replaced by a uniform positively charged background, density $n=\rm const$) with the same local density: 
\begin{equation*}
  \epsilon_{xc}({\bf r};n)\approx\epsilon_{xc}^{LD}(n({\bf r}))
\end{equation*}

The xc potential $V_{xc}$ defined by (\ref{Vxcpot}) is then 
\begin{equation*}
  V_{xc}({\bf r};n)={\delta E_{xc}[n]\over\delta n({\bf r})}= \epsilon_{xc}({\bf r}';n)+ \int n({\bf r}'){\delta \epsilon_{xc}({\bf r}';n)\over\delta n({\bf r})}\d^3r'
\end{equation*}
which in the LDA becomes 
\begin{equation}
  V_{xc}({\bf r};n) =\epsilon_{xc}^{LD}(n)+n{\d \epsilon_{xc}^{LD}(n)\over \d n}= {\d \over \d n}\left(n\epsilon_{xc}^{LD}(n)\right)= V_{xc}^{LD}(n)  \label{Vxcld}
\end{equation}
The xc energy density $\epsilon_{xc}^{LD}$ of the homogeneous gas can be computed exactly\cite{martin}: 
\begin{equation*}
  \epsilon_{xc}^{LD}(n)=\epsilon_x^{LD}(n)+\epsilon_c^{LD}(n)
\end{equation*}
where the $\epsilon_x^{LD}$ is the electron gas exchange term given by\cite{martin} 
\begin{equation*}
  \epsilon_x^{LD}(n)=-{3\over4\pi}(3\pi^2 n)^{1\over3}
\end{equation*}
the rest of $\epsilon_{xc}^{LD}$ is hidden in $\epsilon_c^{LD}(n)$ for which there doesn't exist an analytic formula, but the correlation energies are known exactly from quantum Monte Carlo (QMC) calculations by Ceperley and Alder\cite{pickett}. The energies were fitted by Vosko, Wilkes and Nussair (VWN) with $\epsilon_c^{LD}(n)$ and they got accurate results with errors less than $0.05\rm\,mRy$ in $\epsilon_c^{LD}$, which means that $\epsilon_c^{LD}(n)$ is virtually known exactly. VWN result: 
\begin{equation*}
  \epsilon_c^{LD}(n)\approx {A\over2}\left\{ \ln\left(y^2\over Y(y)\right)+{2b\over Q}\arctan\left(Q\over 2y+b\right)+ \right.
\end{equation*}
\begin{equation*}
  \left. -{by_0\over Y(y_0)}\left[\ln\left((y-y_0)^2\over Y(y)\right) +{2(b+2y_0)\over Q}\arctan\left(Q\over 2y+b\right) \right] \right\}
\end{equation*}
where $y=\sqrt{r_s}$, $Y(y)=y^2+by+c$, $Q=\sqrt{4c-b^2}$, $y_0=-0.10498$, $b=3.72744$, $c=12.93532$, $A=0.0621814$ and $r_s$ is the electron gas parameter, which gives the mean distance between electrons (in atomic units): 
\begin{equation*}
  r_s=\left(3\over4\pi n\right)^{1\over3}
\end{equation*}

The xc potential is then computed from (\ref{Vxcld}): 
\begin{equation*}
  V_{xc}^{LD}=V_x^{LD}+V_c^{LD}
\end{equation*}
\begin{equation*}
  V_x^{LD}=-{1\over\pi}(3\pi^2 n)^{1\over3}
\end{equation*}


\begin{equation*}
  V_c^{LD}={A\over2}\left\{ \ln\left(y^2\over Y(y)\right)+{2b\over Q}\arctan\left(Q\over 2y+b\right)+ \right.
\end{equation*}
\begin{equation*}
  \left. -{by_0\over Y(y_0)}\left[\ln\left((y-y_0)^2\over Y(y)\right) +{2(b+2y_0)\over Q}\arctan\left(Q\over 2y+b\right) \right] \right\}+
\end{equation*}
\begin{equation*}
  -{A\over6}{c(y-y_0)-by_0y\over (y-y_0)Y(y)}
\end{equation*}

Some people also use Perdew and Zunger formulas, but they give essentially the same results. The LDA, although very simple, is suprisingly successful. More sophisticated approximations exist, for example the generalized gradient approximation (GGA), which sometimes gives better results than the LDA, but is not perfect either. Other options include orbital-dependent (implicit) density functionals or a linear response type functionals, but this topic is still evolving. The conclusion is, that the LDA is a good approximation to start with, and only when we are not satisfied, we will have to try some more accurate and modern approximation.

RLDA: Relativistic corrections to the energy-density functional were proposed by MacDonald and Vosko and basically are just a change in $\epsilon_x^{LD}(n)\rightarrow\epsilon_x^{LD}(n)R$: 
\begin{equation*}
  R = \left[1-{3\over2}\left(\beta\mu-\ln(\beta+\mu)\over\beta^2\right)^2\right]
\end{equation*}
where 
\begin{equation*}
  \mu=\sqrt{1+\beta^2}
\end{equation*}
and 
\begin{equation*}
  \beta={(3\pi^2n)^{1\over3}\over c}
\end{equation*}
We also need to calculate these derivatives: 
\begin{equation*}
  {\d R\over\d \beta}= -6{\beta\mu-\ln(\beta+\mu)\over\beta^2}\left({1\over\mu}- {\beta\mu-\ln(\beta+\mu)\over\beta^3}\right)
\end{equation*}
\begin{equation*}
  {\d \beta\over\d n}={\beta\over 3n}
\end{equation*}
\begin{equation*}
  {\d \epsilon_x^{LD}\over\d n}={\epsilon_x^{LD}\over 3n}
\end{equation*}
So 
\begin{equation*}
  V_x^{RLD}=\epsilon_x^{LD}R+n{\d \epsilon_x^{LD}R\over\d n}= {4\over3}\epsilon_x^{LD}R+{1\over3}\epsilon_x^{LD}{\d R\over\d\beta}\beta
\end{equation*}
For $c\to\infty$ we get $\beta\to0$, $R\to1$ and $V_x^{RLD}\to {4\over3}\epsilon_x^{LD}=V_x^{LD}$ as expected, because 
\begin{equation*}
  \lim_{\beta\to0}{\beta\sqrt{1+\beta^2}-\ln(\beta+\sqrt{1+\beta^2})\over \beta^2} = 0
\end{equation*}

\subsection{Iteration to Self-consistency}

The $V_H$ and $V_{xc}$ potentials in the Kohn-Sham equations (\ref{KSeq}) depend on the solution $n$ thus the KS equations need to be iterated to obtain a self-consistent density. One can regard the KS procedure as a nonlinear operator $\hat F$ which satisfies (at the $M$th iteration) 
\begin{equation*}
  n_M^{\rm out}=\hat F n_M
\end{equation*}
and the problem is to find the self-consistent density which satisfies 
\begin{equation*}
  n=\hat F n
\end{equation*}

Due to the long-range nature of the Coulomb interaction, a small change in the input density $n_M$ can lead to a relatively large change in the output density $\hat F n_M$, thus it is not possible to use the output density itself as the input density for the next iteration, since large unstable charge oscillations arise. Rather it is essential to mix input and output densities in an appropriate manner to obtain a new input density.

The $n(\bf r)$ is in practice defined on some grid, or using coefficients of plane waves, local orbitals or the like, which means that the precise relation 
\begin{equation*}
  \one=\int \ket{\bf r}\bra{\bf r}\d^3 r
\end{equation*}
is changed for 
\begin{equation*}
  \one\approx\sum_i \ket{{\bf r}_i}\bra{{\bf r}_i}
\end{equation*}
in the case of a grid (or some other basis like plane waves can be used instead of $\ket{{\bf r}_i}$) and $n({\bf r})=\braket{{\bf r}|n}$ is approximated by $n({\bf r}_i)=\braket{{\bf r}_i|n}$. Let 
\begin{equation*}
  \x=(x_1,x_2,x_3,...),\quad x_i\equiv n({\bf r}_i)=\braket{{\bf r}_i|n}
\end{equation*}
and 
\begin{equation*}
  \F(\x_M)\equiv \hat F n_M, \quad F_i= (\hat F n_M)({\bf r}_i)
\end{equation*}
the self-consistency is reached when $\F(\x)=\x$.

So the problem is in solving the equation 
\begin{equation*}
  \F(\x)=\x
\end{equation*}
where $\x$ denotes a vector in many dimensions (the number of points in the grid). It can also be expressed in the form of the residual $\R(\x)=\F(\x)-\x$ as 
\begin{equation*}
  \R(\x)=0
\end{equation*}
Almost all of the methods start with approximating 
\begin{equation}
  \R(\x_{M+1})-\R(\x_M)\approx\J\cdot(\x_{M+1}-\x_M)  \label{lin}
\end{equation}
where the Jacobian 
\begin{equation*}
  J_{ij}={\partial R_i\over\partial x_j}
\end{equation*}
We want $\R(\x_{M+1})=0$, so substituting that into (\ref{lin}) we get 
\begin{equation*}
  \x_{M+1}\approx\x_M+\J^{-1}\cdot(\R(\x_{M+1})-\R(\x_M))= \x_M-\J^{-1}\cdot\R(\x_M)
\end{equation*}
If we knew the Jacobian exactly, this would be the multidimensional Newton-Raphson method, but we can only make approximations to $\J$ using a sequence of $\J_0$, $\J_1$, $\J_2$, \dots: 
\begin{equation}
  \x_{M+1}=\x_M-\J_M^{-1}\cdot\R(\x_M)  \label{Jaciter}
\end{equation}
and the rate of convergence is determined by the quality of the Jacobian. These type of methods are called quasi-Newton-Raphson methods.

The simplest approach is to use the \textbf{linear mixing} scheme for which 
\begin{equation*}
  \J_M^{-1}=-\alpha\one
\end{equation*}
so 
\begin{equation*}
  \x_{M+1}=\x_M+\alpha\R(\x_M)=\x_M+\alpha(\F(\x_M)-\x_M)
\end{equation*}
where $0<\alpha\le1$ is the mixing parameter, working value is somewhere around $\alpha=0.1$ to $\alpha=0.3$. Unfortunately, this procedure is slow and also we do not explore all the possible densities with this mixing, which means that we don't get the correct density with any accuracy, because we get stuck at a "stiff" situation for which continued iteration does not improve the distance $|\R(\x_M)|$ between input and output densities. On the other hand it's very easy to implement and it works in most cases, although slowly.

Surprisingly very good method is this: 
\begin{equation*}
  \J_M^{-1}=-{\rm diag}(\beta_1,\beta_2,\beta_3,\dots)
\end{equation*}
start with $\beta_1=\beta_2=\beta_3=\cdots=\alpha$ and at every iteration adjust the parameters $\beta_i$ according to this very simple algorithm: if $R_i(\x_{M-1})R_i(\x_M)>0$ then increase $\beta_i$ by $\alpha$ (if $\beta_i>\alpha_{max}$, set $\beta_i=\alpha_{max}$) otherwise set $\beta_i=\alpha$. In my tests it behaves almost as well as the second Broyden method.

More sophisticated approach is the Broyden update, which updates the $\J$ successively at every iteration. The \textbf{first Broyden method} is using this formula: 
\begin{equation*}
  \J_{M+1}=\J_M-{(\Delta\R(\x_M)+\J_M\cdot\Delta\x_M)\Delta\x_M^T\over |\Delta\x_M|^2}
\end{equation*}
which has the disadvantage that we need to compute the inverse Jacobian in (\ref{Jaciter}) at every iteration, which is impossible in our case. The \textbf{second Broyden method} updates the inverse Jacobian directly using this formula 
\begin{equation}
  \J_{M+1}^{-1}=\J_M^{-1}+{(\Delta\x_M-\J_M^{-1}\cdot\Delta\R(\x_M)) \Delta\R(\x_M)^T\over |\Delta\R(\x_M)|^2}  \label{Bupdate}
\end{equation}
starting with the linear mixing: 
\begin{equation*}
  \J_0^{-1}=-\alpha\one
\end{equation*}
It is impossible to store the whole dense matrix of the inverse Jacobian, but fortunately it is not necessary, realising that the (\ref{Bupdate}) has a very simple structure \cite{srivastava}: 
\begin{equation*}
  \J_{M+1}^{-1}=\J_M^{-1}+{\bf u}{\bf v}^T
\end{equation*}
with 
\begin{equation}
  {\bf u}=\Delta\x_M-\J_M^{-1}\cdot\Delta\R(\x_M)  \label{vecu}
\end{equation}
\begin{equation*}
  {\bf v}={\Delta\R(\x_M)\over |\Delta\R(\x_M)|^2}
\end{equation*}
so the whole inverse Jacobian can be written as 
\begin{equation*}
  \J_M^{-1}=-\alpha\one+{\bf u}_1{\bf v}_1^T+{\bf u}_2{\bf v}_2^T+ {\bf u}_3{\bf v}_3^T+\cdots
\end{equation*}
and we only need to know how to apply such a Jacobian to an arbitrary vector, which is needed in (\ref{vecu}) and (\ref{Jaciter}): 
\begin{equation*}
  \J_M^{-1}\cdot\y=-\alpha\y+{\bf u}_1({\bf v}_1^T\y) +{\bf u}_2({\bf v}_2^T\y)+ {\bf u}_3({\bf v}_3^T\y)+\cdots
\end{equation*}
Thus instead of the whole dense matrix, we only need to save the vectors ${\bf u}$ and ${\bf v}$ from every iteration.

Vanderbilt and Louie \cite{vanderbiltlouie} suggested a \textbf{modified Broyden method}, which incorporates weights, but Eyert \cite{eyert} showed that if all the weights are used to tune the iteration process to its fastest convergence, they, in fact, cancel out and the result of the scheme is called by Eyert the \textbf{generalized Broyden method}, whose scheme shown by Eyert is exactly the same as for the \textbf{Anderson mixing}: 
\begin{equation*}
  \sum_{p=M-k}^{M-1}(1+\omega_0^2\delta_{pn})\Delta\R(\x_n)^T\Delta\R(\x_p) \gamma_p =\Delta\R(\x_n)^T\R(\x_M)
\end{equation*}
\begin{equation*}
  \x_{M+1}=\x_M+\beta_M\R(\x_M)-\sum_{p=M-k}^{M-1}\gamma_p (\Delta\x_p+\beta_M\Delta\R(\x_p))
\end{equation*}
which according to Eyert should converge even faster than the second Broyden method, but it doesn't in my own implementation. $\omega_0$ is added just for a numerical stability, good value is $\omega=0.01$, but it can also be switched off by $\omega_0=0$. $p$ is the number of last iterations to use, good value according to Eyert is $p=5$, $\beta_M$ shouldn't influence the convergence much for $p=5$.

The problem with $n$ is that there are two conditions which need to be satisfied 
\begin{equation*}
  n>0
\end{equation*}
and the normalization 
\begin{equation*}
  \int n({\bf r}) \d^3r=Z
\end{equation*}
The Newton method converges to the correct norm, but slowly. The condition $n>0$ however causes great instability. One option could be to use a logistic function like 
\begin{equation*}
  n(r)={C\over 1+e^{-x(r)}}
\end{equation*}
for sufficiently large $C$ and solve for $x$, which can be both positive and negative. But more elegant solution is to mix $V_h+V_{xc}$ instead of the densities.

\subsection{Example: Pb Atom}

To illustrate the explained theory, we will show how to calculate the Pb atom. We have $N=82$ and 
\begin{equation*}
  v({\bf r}_i)=-{82\over |{\bf r}_i|}
\end{equation*}
and we need to sum over the lowest 82 eigenvalues in (\ref{KSdensity}). One option (the correct one) is to automatically try different "n" and "l" until we are sure we got the lowest 82 energies. But for Pb the combination of "n" and "l" is well-known, it is (first number is $n$, the letter is $l$ and the number in superscript gives the number of times the particular eigenvalue needs to be taken into account in the sum): $1S^2$, $2S^2$, $2P^6$, $3S^2$, $3P^6$, $3D^{10}$, $4S^2$, $4P^6$, $4D^{10}$, $4F^{14}$, $5S^2$, $5P^6$, $5D^{10}$, $6S^2$, $6P^2$ (notice the 5F and 5G are missing). Together it is 82 eigenvalues. The KS energies for these eigenvalues are:

-2701.6 -466.18 -471.87 -111.45 -108.24 -92.183 -24.498 -22.086 -15.119 -5.6606 -3.9570 -2.9743 -.92718 -.33665 -.14914

\section{Pseudopotentials}

\subsection{Introduction}

Literature about pseudopotentials is unfortunately scattered among many arcticles, so this section gives a review and should save the reader from a lot of troubles.

\subsection{Hermitian Operators in Spherical Symmetry}

We show that every Hermitian operator $\hat V$ in the spherical symmetric problem ($\hat V=R^{-1}\hat VR$) can be written in the form 
\begin{equation}
  \hat V=\sum_{lm}\ket{lm}\hat V_l\bra{lm}  \label{lmexpansion}
\end{equation}
where the operator $\hat V_l=\braket{lm|\hat V|lm}$ has matrix elements 
\begin{equation*}
  \braket{\rho|\hat V_l|\rho'}=\bra{lm}\braket{\rho|\hat V|\rho'}\ket{lm}= V_l(\rho,\rho')
\end{equation*}
{\bf Proof:} Matrix elements of a general Hermitian operator $\hat V$ are 
\begin{equation*}
  \braket{{\bf r}|\hat V|\varphi}= \int\braket{{\bf r}|\hat V|{\bf r'}}\braket{{\bf r'}|\varphi}\d^3r'= \int V({\bf r},{\bf r'})\varphi({\bf r'})\d^3r'
\end{equation*}
where 
\begin{equation*}
  V({\bf r}, {\bf r'})=\braket{{\bf r}|\hat V|{\bf r'}}
\end{equation*}
In spherical symmetry, we have 
\begin{equation*}
  \braket{{\bf r}|\hat V|\varphi} =\braket{{\bf r}|R^{-1}\hat VR|\varphi} =\braket{{\bf r}|R^{\dagger}\hat VR|\varphi} =\int\braket{{\bf r}|R^{\dagger}\hat VR|{\bf r'}}\braket{{\bf r'}|\varphi}\d^3r' =
\end{equation*}
\begin{equation*}
  =\int\braket{R{\bf r}|\hat V|R{\bf r'}}\braket{{\bf r'}|\varphi}\d^3r' =\int V(R{\bf r},R{\bf r'})\varphi({\bf r'})\d^3r'
\end{equation*}
where $R$ is the rotation operator (it's unitary). We have thus derived $V(R{\bf r},R{\bf r'})=V({\bf r},{\bf r'})$ true for any $R$, which means that the the kernel only depends on $\rho$, $\rho'$ and ${\bf\hat r}\cdot{\bf\hat r'}$, where ${\bf r}=\rho{\bf\hat r}$ and ${\bf r'}=\rho'{\bf\hat r'}$. So we obtain using (\ref{fylm})
\begin{equation*}
  V({\bf r}, {\bf r'})=V(\rho,\rho',{\bf\hat r}\cdot{\bf\hat r'})= \sum_{lm} Y_{lm}({\bf\hat r}) V_l(\rho,\rho') Y_{lm}^*({\bf\hat r'})
\end{equation*}
where 
\begin{equation*}
  V_l(\rho,\rho')={(2l+1)^2\over8\pi}\int_{-1}^1 P_l(x)V_l(\rho,\rho',x)\d x
\end{equation*}
In Dirac notation: 
\begin{equation*}
  V({\bf r}, {\bf r'})=\braket{{\bf r}|\hat V|{\bf r'}} =\bra{\bf\hat r}\braket{\rho|\hat V|\rho'}\ket{\bf\hat r'} =\sum_{lml'm'}\braket{{\bf\hat r}|lm}\bra{lm}\braket{\rho|\hat V|\rho'} \ket{l'm'}\braket{l'm'|\bf\hat r'}
\end{equation*}
From the above derivation we see that we must have: 
\begin{equation*}
  \bra{lm}\braket{\rho|\hat V|\rho'}\ket{l'm'}= V_l(\rho,\rho')\delta_{ll'}\delta_{mm'}
\end{equation*}
in other words 
\begin{equation}
  V_l(\rho,\rho')=\bra{lm}\braket{\rho|\hat V|\rho'}\ket{lm}  \label{vlm2}
\end{equation}
so we get 
\begin{equation*}
  \braket{{\bf r}|\hat V|{\bf r'}} =\sum_{lm}\braket{{\bf\hat r}|lm}V_l(\rho,\rho')\braket{lm|\bf\hat r'} =\sum_{lm}Y_{lm}(\theta,\phi) V_l(\rho,\rho') Y_{lm}^*(\theta',\phi')
\end{equation*}
and 
\begin{equation*}
  \hat V =\sum_{lm}\ket{lm}\braket{lm|\hat V|lm}\bra{lm} =\sum_{lm}\ket{lm}\hat V_l\bra{lm}
\end{equation*}
where the operator $\hat V_l=\braket{lm|\hat V|lm}$ only acts on the radial part of the wavefunction and according to (\ref{vlm2}) it doesn't depend on $m$. Also according to (\ref{vlm2}) its matrix elements are 
\begin{equation*}
  \braket{\rho|\hat V_l|\rho'}=\bra{lm}\braket{\rho|\hat V|\rho'}\ket{lm}= V_l(\rho,\rho')
\end{equation*}

\subsection{Nonlocal Pseudopotentials}

A nonlocal pseudopotential $\hat V$ is just a general Hermitian operator. We
only want to construct pseudopotentials in the spherical problem, so every
pseudopotential can be written in the form (\ref{lmexpansion}). In practice we
only use either {\it local} (the operator $\hat V$ is local) or {\it semilocal}
(the operator $\hat V$ is radially local, but angularly nonlocal) pseudopotential.

Local potential (radially and angularly local) is defined by: 
\begin{equation*}
  \braket{{\bf r}|\hat V|{\bf r'}}=V(\rho)\braket{{\bf r}|{\bf r'}}
\end{equation*}
so we can simply write 
\begin{equation}
  \hat V=V(\rho)  \label{loc1}
\end{equation}
so 
\begin{equation*}
  V_l(\rho,\rho') =\bra{lm}\braket{\rho|\hat V|\rho'}\ket{lm} =V(\rho)\braket{\rho|\rho'} =V(\rho){\delta(\rho-\rho')\over\rho^2}
\end{equation*}
so it turned out that the kernel is local and doesn't depend on $l$ and we get 
\begin{equation*}
  V({\bf r}(\rho,\theta,\phi), {\bf r'}(\rho',\theta',\phi'))= \sum_{lm}Y_{lm}(\theta,\phi) V(\rho){\delta(\rho-\rho')\over\rho^2} Y_{lm}^*(\theta',\phi')=
\end{equation*}
\begin{equation*}
  =V(\rho){1\over\rho^2\sin\theta} \delta(\rho-\rho')\delta(\theta-\theta')\delta(\phi-\phi')= V(\rho)\delta({\bf r}-{\bf r}')
\end{equation*}
and 
\begin{equation*}
  \braket{{\bf r}|\hat V|\varphi}=\int V(\rho)\delta({\bf r}-{\bf r}') \varphi({\bf r'})\d^3r'=V(\rho)\varphi({\bf r})
\end{equation*}
so we recover (\ref{loc1}). But we are just fooling around, there's nothing new in these formulas.

For a semilocal potential (radially local, but angularly nonlocal), the kernel cannot depend on $m$ and is radially local, so: 
\begin{equation*}
  \braket{\rho|\hat V_l|\rho'}=V_l(\rho,\rho') =\bra{lm}\braket{\rho|\hat V|\rho'}\ket{lm} =V_l(\rho)\braket{\rho|\rho'} =V_l(\rho){\delta(\rho-\rho')\over\rho^2}
\end{equation*}
so the kernel is local and does depend on $l$ and we simply write 
\begin{equation*}
  \hat V_l=V_l(\rho)
\end{equation*}
and 
\begin{equation}
  \hat V=\sum_{lm} \ket{lm}V_l(\rho)\bra{lm}  \label{semi}
\end{equation}
We can also calculate the same result explicitly in the $\bf r$ representation: 
\begin{equation*}
  V({\bf r}(\rho,\theta,\phi), {\bf r'}(\rho',\theta',\phi'))= \sum_{lm}Y_{lm}(\theta,\phi) V_l(\rho){\delta(\rho-\rho')\over\rho^2} Y_{lm}^*(\theta',\phi')
\end{equation*}
and 
\begin{equation*}
  \braket{{\bf r}|\hat V|\varphi}=\int \sum_{lm}Y_{lm}(\theta,\phi) V_l(\rho){\delta(\rho-\rho')\over\rho^2} Y_{lm}^*(\theta',\phi') \varphi({\bf r'})\d^3r'=
\end{equation*}
\begin{equation*}
  = \sum_{lm}Y_{lm}(\theta,\phi) V_l(\rho) \int Y_{lm}^*(\theta',\phi') \varphi(\rho{\bf\hat r'})\d\Omega'
\end{equation*}
or in Dirac notation 
\begin{equation*}
  \braket{{\bf r}|\hat V|\varphi}= \sum_{lm} \braket{{\bf\hat r}|lm}V_l(\rho) \bra{lm}\braket{\rho|\varphi}
\end{equation*}
and we recover (\ref{semi}).

So, to sum it up: semilocal pseudopotential is a general hermitian operator in the spherically symmetric problem (i.e. $\hat V=R^{-1}\hat VR$) and radially local. All such operators can be written in the form (\ref{semi}).

Now, it can be shown that if we make the assumption of radial locality, we get "correct" wavefunctions and energies in the linear approximation. We generally only take a few terms in the expansion (\ref{semi}), usually only $V_0$, $V_1$ and $V_2$, sometimes also $V_3$ and $V_4$.

\subsection{Separable Potentials}

The pseudopotential above (Hamman, Schlüter, Chiang) has the form 
\begin{equation*}
  \hat V=\sum_{lm} \ket{lm}V_l(\rho)\bra{lm} =V_{loc}(\rho)+\sum_{lm} \ket{lm}[V_l(\rho)-V_{loc}(\rho)]\bra{lm}
\end{equation*}
Or, equivalently, in the $\bf r$ representation: 
\begin{equation*}
  V({\bf r},{\bf r'})=\braket{{\bf r}|\hat V|{\bf r'}}= V_{loc}(\rho)\delta({\bf r}-{\bf r'})+{\delta(\rho-\rho')\over\rho^2} \sum_{lm}Y_{lm}({\bf\hat r})[V_l(\rho)-V_{loc}(\rho)]Y_{lm}^*({\bf\hat r'})
\end{equation*}
The first term doesn't cause a problem. Let's denote the second term (which is semilocal) simply by $v$: 
\begin{equation*}
  v=\sum_{lm} \ket{lm}[V_l(\rho)-V_{loc}(\rho)]\bra{lm}
\end{equation*}
Let's choose a complete but otherwise arbitrary set of functions $\ket{\phi_i}$ (they contain both a radial and an angular dependence) and define a matrix $U$ is by the equation 
\begin{equation*}
  \sum_j U_{ij}\braket{\phi_j|v|\phi_k}=\delta_{ik}
\end{equation*}
then ($\ket{\psi}=\ket{\phi_k}\alpha_k$): 
\begin{equation*}
  v\ket{\psi} =\sum_{ik}v\ket{\phi_i}\delta_{ik}\alpha_k =\sum_{ijk}v\ket{\phi_i}U_{ij}\braket{\phi_j|v|\phi_k}\alpha_k =\sum_{ij}v\ket{\phi_i}U_{ij}\braket{\phi_j|v|\psi}
\end{equation*}
So any Hermitian operator (including $v$) can be transformed exactly into the following form 
\begin{equation*}
  v=\sum_{ij}v\ket{\phi_i}U_{ij}\bra{\phi_j}v
\end{equation*}
We diagonalize the matrix $U$ by choosing such functions $\ket{\bar\phi_i}$ for which the matrix $\braket{\bar\phi_j|v|\bar\phi_k}$ (and hence the corresponding matrix $U$) is equal to \one. We can find such functions for example using the Gram-Schmidt orthogonalization procedure on $\ket{\phi_i}$ with a norm $\braket{f|v|g}$ (for functions $f$ and $g$), more on that later. Then 
\begin{equation}
  v =\sum_{i}v\ket{\bar\phi_i}{1\over\braket{\bar\phi_i|v|\bar\phi_i}} \bra{\bar\phi_i}v =\sum_{i}v\ket{\bar\phi_i}\bra{\bar\phi_i}v  \label{vsep}
\end{equation}
We could take any $\ket{\phi_i}$ and orthogonalize them. But because we have $v$ in the form of (\ref{semi}), we will be using $\ket{\phi_i}$ in the form $\ket{\phi_i}=\ket{R_{nl}}\ket{lm}$, because it turns out we will only need to orthogonalize the radial parts. The first term in (\ref{vsep}) then corresponds to the KB potential. We of course take more terms and get accurate results without ghost states.

Let's look at the orthogonalization. We start with the wavefunctions: 
\begin{equation*}
  \ket{\phi_i}=\ket{R_{nl}}\ket{lm}
\end{equation*}
where $R_{nl}(\rho)=\braket{\rho|R_{nl}}$ and $i$ goes over all possible triplets $(nlm)$.

We can also relate the $i$ and $n$, $l$, $m$ using this formula
\begin{equation*}
  i_{nlm}=\sum_{k=1}^{n-1}k^2+\left(\sum_{k=0}^{l-1} (2k+1)\right) + (l+m+1)= {(n-1)n(2n-1)\over6} + l(l+1)+m+1
\end{equation*}

The operator $v$ acts on these $\ket{\phi_i}$ like this 
\begin{equation*}
  \braket{{\bf r}|v|\phi_i} =\braket{{\bf r}|v|R_{nl}}\ket{lm} =\bra{{\bf\hat r}}\braket{\rho|V_l(\rho)|R_{nl}}\ket{lm} =V_l(\rho)R_{nl}(\rho)Y_{lm}({\bf\hat r})
\end{equation*}
Now we need to construct new orthogonal set of functions $\ket{\bar\phi_i}$ satisfying 
\begin{equation*}
  \braket{\bar\phi_i|v|\bar\phi_j}=\delta_{ij}
\end{equation*}
This can be done using several methods, we chose the Gram-Schmidt orthogonalization procedure, which works according to the following scheme: 
\begin{eqnarray*}
\ket{\tilde\phi_1}&=&\one{1\over\sqrt{\braket{\phi_1|v|\phi_1}}}\ket{\phi_1} ;\quad\quad \quad\quad\quad\quad\quad \quad\quad\quad\quad\quad \ket{\bar\phi_1}={1\over\sqrt{\braket{\tilde\phi_1|v|\tilde\phi_1}}} \ket{\tilde\phi_1} \\
\ket{\tilde\phi_2}&=& \left(\one -\ket{\bar\phi_1}\bra{\bar\phi_1}v \right){1\over\sqrt{\braket{\phi_2|v|\phi_2}}}\ket{\phi_2};\quad\quad \quad\quad\quad\quad\quad \ket{\bar\phi_2}={1\over\sqrt{\braket{\tilde\phi_2|v|\tilde\phi_2}}} \ket{\tilde\phi_2} \\
\ket{\tilde\phi_3}&=& \left(\one -\ket{\bar\phi_1}\bra{\bar\phi_1}v -\ket{\bar\phi_2}\bra{\bar\phi_2}v \right){1\over\sqrt{\braket{\phi_3|v|\phi_3}}}\ket{\phi_3};\quad\quad \ket{\bar\phi_3}={1\over\sqrt{\braket{\tilde\phi_3|v|\tilde\phi_3}}} \ket{\tilde\phi_3} \\
\dots& \\
\end{eqnarray*}
 We can verify by a direct calculation that this procedure ensures 
\begin{equation*}
  \braket{\bar\phi_i|v|\bar\phi_j}=\delta_{ij}
\end{equation*}
It may be useful to compute the normalization factors explicitly: 
\begin{eqnarray*}
\braket{\tilde\phi_1|v|\tilde\phi_1}&=&1 \\
\braket{\tilde\phi_2|v|\tilde\phi_2}&=&1 -{\braket{\phi_2|v|\bar\phi_1}\braket{\bar\phi_1|v|\phi_2} \over\braket{\phi_2|v|\phi_2}} \\
\braket{\tilde\phi_3|v|\tilde\phi_3}&=&1 -{\braket{\phi_3|v|\bar\phi_1}\braket{\bar\phi_1|v|\phi_3}+ \braket{\phi_3|v|\bar\phi_2}\braket{\bar\phi_2|v|\phi_3} \over\braket{\phi_3|v|\phi_3}} \\
...& \\
\end{eqnarray*}
 we can also write down a first few orthogonal vectors explicitly: 
\begin{eqnarray*}
\ket{\bar\phi_1}&=&{\ket{\phi_1}\over\sqrt{\braket{\phi_1|v|\phi_1}}} \\
\ket{\bar\phi_2}&=&{\ket{\phi_2}\braket{\phi_1|v|\phi_1}-\ket{\phi_1}\braket{\phi_1|v|\phi_2} \over\sqrt{(\braket{\phi_1|v|\phi_1}\braket{\phi_2|v|\phi_2}-\braket{\phi_2|v|\phi_1}\braket{\phi_1|v|\phi_2})\braket{\phi_1|v|\phi_1}\braket{\phi_2|v|\phi_2}}} \\
\end{eqnarray*}
 Now the crucial observation is 
\begin{equation*}
  \bra{lm}\braket{R_{nl}|v|R_{n'l'}}\ket{l'm'}= \braket{R_{nl}|V_l(\rho)|R_{n'l}}\delta_{ll'}\delta_{mm'}
\end{equation*}
which means that $\braket{\phi_i|v|\phi_j}=0$ if $\ket{\phi_i}$ and $\ket{\phi_j}$ have different $l$ or $m$. In other words $\ket{\phi_i}$ and $\ket{\phi_j}$ for different $\ket{lm}$ are already orthogonal. Thus the G-S orthogonalization procedure only makes the $R_{nl}$ orthogonal for the same $\ket{lm}$. To get explicit expressions for $\ket{\bar\phi_i}$, we simply use the formulas above and get: 
\begin{equation*}
  \ket{\phi_i}=\ket{R_{nl}}\ket{lm}\quad\to\quad \ket{\bar\phi_i}=\ket{\bar R_{nl}}\ket{lm}
\end{equation*}
where we have constructed new $\ket{\bar R_{nl}}$ from original $\ket{R_{nl}}$: 
\begin{eqnarray*}
\ket{\bar R_{10}}&=&{\ket{R_{10}}\over\sqrt{\braket{R_{10}|V_0|R_{10}}}} \\
\ket{\bar R_{20}}&=&{\ket{R_{20}} -\ket{\bar R_{10}}\braket{\bar R_{10}|V_0|R_{20}}\over\sqrt{\dots}} \\
\ket{\bar R_{21}}&=&{\ket{R_{21}}\over\sqrt{\braket{R_{21}|V_1|R_{21}}}} \\
\ket{\bar R_{30}}&=&{\ket{R_{30}} -\ket{\bar R_{10}}\braket{\bar R_{10}|V_0|R_{30}} -\ket{\bar R_{20}}\braket{\bar R_{20}|V_0|R_{30}} \over\sqrt{\dots}} \\
\ket{\bar R_{31}}&=&{\ket{R_{31}} -\ket{\bar R_{21}}\braket{\bar R_{21}|V_1|R_{31}} \over\sqrt{\dots}} \\
\ket{\bar R_{32}}&=&{\ket{R_{32}}\over\sqrt{\braket{R_{32}|V_1|R_{32}}}} \\
\ket{\bar R_{40}}&=&{\ket{R_{40}} -\ket{\bar R_{10}}\braket{\bar R_{10}|V_0|R_{40}} -\ket{\bar R_{20}}\braket{\bar R_{20}|V_0|R_{40}} -\ket{\bar R_{30}}\braket{\bar R_{30}|V_0|R_{40}} \over\sqrt{\dots}} \\
\ket{\bar R_{41}}&=&{\ket{R_{41}} -\ket{\bar R_{21}}\braket{\bar R_{21}|V_1|R_{41}} -\ket{\bar R_{31}}\braket{\bar R_{31}|V_1|R_{41}} \over\sqrt{\dots}} \\
&\dots& \\
\end{eqnarray*}
 Ok, so we have constructed new $\ket{\bar R_{nl}}$ from $\ket{R_{nl}}$ which obey 
\begin{equation}
  \braket{\bar R_{nl}|V_l|\bar R_{n'l}}=\delta_{nn'}  \label{orthog}
\end{equation}
so for every $V_l$, we construct $\ket{\bar R_{nl}}$ for $n=l+1,\,\, l+2, \cdots$. Let's continue: 
\begin{equation*}
  v\ket{\bar\phi_i}=V_l(\rho)\ket{\bar R_{nl}}\ket{lm}
\end{equation*}
and finally we arrive at the separable form of the $l$ dependent pseudopotential 
\begin{equation}
  v =\sum_{i}v\ket{\bar\phi_i}\bra{\bar\phi_i}v =\sum_{i}V_l(\rho)\ket{\bar R_{nl}}\ket{lm} \bra{lm}\bra{\bar R_{nl}}V_l(\rho)  \label{Vsep}
\end{equation}
Note: the $V_l$ is actually $V_l-V_{loc}$, but this is just a detail.

To have some explicit formula, let's write how the separable potential acts on a wavefunction: 
\begin{equation*}
  (v\psi)({\bf r})=\braket{{\bf r}|v|\psi}= \sum_i\braket{{\bf\hat r}|lm}\braket{\rho|V_l(\rho)|\bar R_{nl}} \bra{\bar R_{nl}}V_l(\rho)\braket{lm|\psi}=
\end{equation*}
\begin{equation*}
  =\sum_iY_{lm}({\bf\hat r})\bar R_{nl}(\rho)V_l(\rho) \int \bar R_{nl}(\rho')V_l(\rho')\int Y_{lm}^*({\bf\hat r'})\psi({\bf r'})\,\d \Omega'\,\rho'^2 \d\rho'=
\end{equation*}
\begin{equation*}
  =\sum_iY_{lm}({\bf\hat r})\bar R_{nl}(\rho)V_l(\rho) \int \bar R_{nl}(\rho')V_l(\rho') Y_{lm}^*({\bf\hat r'})\psi({\bf r'})\,\d^3r'
\end{equation*}

To have some insight on what we are actually doing: we are making the local potential $V_l$ nonlocal using: 
\begin{equation}
  V_l=\sum_{n=l+1}^\infty V_l\ket{\bar R_{nl}}\bra{\bar R_{nl}}V_l  \label{Vlsep}
\end{equation}
where 
\begin{equation*}
  \braket{\bar R_{nl}|V_l|\bar R_{n'l}}=\delta_{nn'}
\end{equation*}
or in ${\bf r}$ representation: 
\begin{equation*}
  V_l(\rho)\psi(\rho {\bf\hat r})=\sum_n V_l(\rho)\bar R_{nl}(\rho) \int\bar R_{nl}(\rho')V_l(\rho')\psi(\rho'{\bf\hat r})\rho'^2\d \rho'
\end{equation*}
which is useful when computing integrals of this type 
\begin{equation*}
  V_{ij} =\int\phi_i(\rho) V_l \phi_j(\rho) \rho^2\d^3 \rho =\braket{i|V_l|j}= \sum_n \braket{i|V_l|{\bar R_{nl}}}\braket{\bar R_{nl}|V_l|j}
\end{equation*}
\begin{equation*}
  \braket{i|V_l|{\bar R_{nl}}}=\int\phi_i(\rho)V_l(\rho)\bar R_{nl}(\rho) \rho^2\d\rho
\end{equation*}
because the integral on the left hand side actually represents $N^2$ integrals, where $N$ is the number of basis vectors $\ket{\phi_i}$. The sum on the right hand side however only represents $K\cdot N$ integrals, where $K$ is the number of terms taken into account in (\ref{Vlsep}). Of course taking only finite number of terms in (\ref{Vlsep}) is only an approximation to $\hat V_l$. In our case, we don't need these 1D integrals (which can be easily computed directly, because $V_l$ is local and the basis functions $\phi_i$ are nonzero only around a node in the mesh, which means that the matrix $V_{ij}$ is sparse), but 3D integrals, where angular parts of $V$ are nonlocal and radial part is local (so the matrix $V_{ij}$ is dense), so the above procedure is the only way how to proceed, because we decompose the matrix $V_{ij}$ into the sum of matrices in the form $p_ip_j^*$, which can easily be handled and solved.

The scheme for the separation described above works for any functions $R_{nl}(\rho)$. Because of the form of the expansion (\ref{Vlsep}) however, we will use $R_{nl}$ from one atomic calculation. We need to approximate $V_l$ by as few terms as possible, so imagine how the $V_l(\rho)$ acts on the lowest radial function in the $l$ subspace, which is $\ket{R_{l+1;l}}$ and we see that all the terms in (\ref{Vlsep}) except the first one $V_l\ket{\bar R_{l+1;l}}\bra{\bar R_{l+1;l}}V_l$ give zero, because they are orthogonal to $\ket{R_{l+1;l}}$. For the function $\ket{R_{l+2;l}}$ all the terms except the first two are zero, because $\braket{\bar R_{nl}|V_0|R_{l+2;l}}\neq0$ only for $n=l+1$ or $n=l+2$ (because the vectors $\ket{R_{l+1;l}}$ and $\ket{R_{l+2;l}}$ span the same subspace as $\ket{\bar R_{l+1;l}}$ and $\ket{\bar R_{l+2;l}}$ and using (\ref{orthog})) For functions, which are a little different from all $\ket{R_{nl}}$ ($n>l$), we won't genereally get precise results taking any (finite) number of terms in (\ref{Vlsep}), but the higher terms should give smaller and smaller corrections.

So, to sum it up: We take all the $V_l$ in (\ref{Vsep}) as we did in (\ref{semi}). Theoretically we should take $\bar R_{nl}$ for all $n=l+1,\,\, l+2,\,\,l+3,\dots$, but practically it suffices to only take several $\bar R_{nl}$ for a given $l$ from one atomic calculaction.

Let's give an example: we are calculating 14 electrons, so we will only take into account the lowest 14 eigenvalues in the Kohn sham equations, which are $\ket{\phi_1}$ up to $\ket{\phi_{14}}$. The lowest radial functions in each $l$ subspace are $\ket{\phi_i}$ for $i=1,3,4,5,10,11,12,13,14$ and on these 9 functions we get a precise result with only one term in the expansion (\ref{Vlsep}). For the other 5 functions ($i=2,6,7,8,9$) we will have to take into account more terms. Let's look in more detail at the case $l=0$ (i.e. $i=1,2,6$). Then 
\begin{equation*}
  V_0= V_0\ket{\bar R_{10}}\bra{\bar R_{10}}V_0+ V_0\ket{\bar R_{20}}\bra{\bar R_{20}}V_0+ V_0\ket{\bar R_{30}}\bra{\bar R_{30}}V_0+ \dots
\end{equation*}
and for the case $i=1$ we see that one term in (\ref{Vlsep}) is enough: 
\begin{equation*}
  v\ket{\phi_1}=v\ket{R_{10}}\ket{00}=V_0\ket{R_{10}}\ket{00}= V_0\ket{\bar R_{10}}\bra{\bar R_{10}}V_0\ket{R_{10}}\ket{00}
\end{equation*}
because $\braket{\bar R_{n0}|V_0|R_{10}}=0$ for $n>1$. For the case $i=2$ we get the correct result with 2 terms in (\ref{Vlsep})
\begin{equation*}
  v\ket{\phi_2}=v\ket{R_{20}}\ket{00}=V_0\ket{R_{20}}\ket{00}=( V_0\ket{\bar R_{10}}\bra{\bar R_{10}}V_0\ket{R_{20}}+ V_0\ket{\bar R_{20}}\bra{\bar R_{20}}V_0\ket{R_{20}} )\ket{00}
\end{equation*}
because $\braket{\bar R_{n0}|V_0|R_{20}}=0$ for $n>2$. For the case $i=6$ we need to take into account 3 terms etc. We can see from this example, that taking $\ket{R_{nl}}$ from one atomic calculation, we get precise results (with the same atom) only taking into account a finite number of terms in (\ref{Vlsep}), for 14 electrons actually only 3 terms. For several atoms calculation, we won't get precise results, but it should be a sufficiently good approximation.

The described method is general, the only drawback is that if we don't take functions $\ket{R_{nl}}$ which are similar to the solution, we need to take a lot of terms in (\ref{Vlsep}), resulting in many matrices of the form $p_ip_j^*$, which we don't want, even though, theoretically we can get a solution with any precision we want taking more and more terms in (\ref{Vlsep}).

See also \cite{blochl}.
